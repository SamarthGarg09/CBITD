{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ef1cf4-6800-47e0-b90f-d7f3e3c6b57c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: groq in /home/2017025/sgarg01/.python3-3.10-torch200/site-packages/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/2017025/sgarg01/.python3-3.10-torch200/site-packages/lib/python3.10/site-packages (from groq) (3.6.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/2017025/sgarg01/.python3-3.10-torch200/site-packages/lib/python3.10/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/2017025/sgarg01/.python3-3.10-torch200/site-packages/lib/python3.10/site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/2017025/sgarg01/.python3-3.10-torch200/site-packages/lib/python3.10/site-packages (from groq) (1.10.7)\n",
      "Requirement already satisfied: sniffio in /home/2017025/sgarg01/.python3-3.10-torch200/site-packages/lib/python3.10/site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/2017025/sgarg01/.python3-3.10-torch200/site-packages/lib/python3.10/site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /soft/AIDL/conda_envs/pytorch-200/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (3.4)\n",
      "Requirement already satisfied: certifi in /home/2017025/sgarg01/.python3-3.10-torch200/site-packages/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/2017025/sgarg01/.python3-3.10-torch200/site-packages/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/2017025/sgarg01/.python3-3.10-torch200/site-packages/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f59f540c-89b4-41d3-9ee1-5cbda5a5eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export GROQ_API_KEY=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70fa6bdf-69dd-4ed5-bb8f-bb443f418e76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed6c59bb91645d3aa1658077d75cea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Samples:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10 records to 'misclassification_word_explanations.json'\n",
      "Saved 20 records to 'misclassification_word_explanations.json'\n",
      "Saved 30 records to 'misclassification_word_explanations.json'\n",
      "Saved 40 records to 'misclassification_word_explanations.json'\n",
      "Saved 50 records to 'misclassification_word_explanations.json'\n",
      "Process complete. Results saved to 'misclassification_word_explanations.json'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from groq import Groq\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "client = Groq(api_key='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') \n",
    "df = pd.read_csv('misclassified_samples/ms_cg.csv')\n",
    "\n",
    "sentences = df['sentence'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "incorrect_predictions = [1 - int(label) for label in labels]  \n",
    "def save_to_json(data, file_name='toxic_not_profane_sentences.json'):\n",
    "    \"\"\"Save data to a JSON file.\"\"\"\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "results = []\n",
    "\n",
    "num_samples = 50  \n",
    "for idx in tqdm(range(num_samples), desc=\"Processing Samples\"):\n",
    "\n",
    "    if idx >= len(sentences):\n",
    "        print(f\"Reached the end of the dataset at index {idx}.\")\n",
    "        break\n",
    "\n",
    "    sentence = sentences[idx]\n",
    "    incorrect_prediction = incorrect_predictions[idx]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    The task is to analyze the following sentence and its incorrect classification as either normal (0) or toxic (1). Your objective is to identify the specific words in the sentence that may have contributed to this misclassification. The output should be in the following JSON format:\n",
    "\n",
    "    ```\n",
    "    {{\n",
    "        \"sentence\": \"{sentence}\",\n",
    "        \"incorrect_prediction\": {incorrect_prediction},\n",
    "        \"words\": [w1, w2, ...]\n",
    "    }}\n",
    "    ```\n",
    "\n",
    "    - List only the relevant words that influenced the incorrect prediction.\n",
    "    - Use without any additional text, numbers, explanations, or punctuation.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-8b-8192\"  # Ensure this is the correct model name\n",
    "        )\n",
    "\n",
    "        response = chat_completion.choices[0].message.content.strip()\n",
    "\n",
    "        try:\n",
    "            response_json = json.loads(response)\n",
    "            words = ','.join(response_json.get(\"words\", []))\n",
    "        except json.JSONDecodeError:\n",
    "            words_line = next((line for line in response.split('\\n') if line.strip().startswith('\"words\":')), None)\n",
    "            if words_line:\n",
    "                start = words_line.find('[') + 1\n",
    "                end = words_line.find(']')\n",
    "                words = words_line[start:end].replace('\"', '').replace(' ', '')\n",
    "            else:\n",
    "                words = \"\"\n",
    "\n",
    "        results.append({\n",
    "            \"sentence\": sentence,\n",
    "            \"incorrect_prediction\": incorrect_prediction,\n",
    "            \"words\": words\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sentence {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if (idx + 1) % 10 == 0:\n",
    "        save_to_json(results, file_name='misclassification_word_explanations.json')\n",
    "        print(f\"Saved {len(results)} records to 'misclassification_word_explanations.json'\")\n",
    "\n",
    "if len(results) % 10 != 0:\n",
    "    save_to_json(results, file_name='misclassification_word_explanations.json')\n",
    "    print(f\"Saved final {len(results)} records to 'misclassification_word_explanations.json'\")\n",
    "\n",
    "print(\"Process complete. Results saved to 'misclassification_word_explanations.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ddd2cbba-5c73-4380-8091-08d16e4409f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of All Words:\n",
      "['beaner', 'drunk', 'ðŸ˜‚', 'ðŸ˜‚', 'ðŸ˜‚', 'raped', 'retarded', 'pakistani', 'paki', 'divorced', 'married', 'dyke', 'white', 'trash', 'collapsing', 'faggot', 'heeb', 'muramasa', 'jihadi', 'nigger', 'jewish', 'holocaust', 'negation', 'dykes', 'autistic', 'retarded', 'negro', 'beat', 'james', 'pimped', 'black', 'gook', 'fuck', 'fuck', 'paki', 'raping', 'degeneracy', 'ching', 'chong', 'moslem', 'slave', 'niggers', 'muzzies', 'spics', 'fuck', 'chinks', 'niggers', 'redskins', 'racist', 'ugly', 'dykes', 'moslems', 'dhimmi', 'un', 'moslem', 'niggerz', 'anti', 'gay', 'bigot', 'faggots', 'goy', 'dindu', 'kill', 'white', 'king', 'kike', 'genetic', 'jew', 'white', 'nations', 'dyke', 'dyke', 'big', 'mohajirs', 'slums', 'karachi', 'goons', 'kill', 'mulla', 'kill', 'raped', 'placating', 'shouters', 'anti', 'sheboon', 'ebonics', 'nigga', 'shoot', 'kill', 'lawless', 'shithole', 'peasant', 'coming', 'for', 'your', 'dollars', 'nuclear', 'holocaust', 'nazi', 'puke', 'sacks', 'white suppressiom', 'supremacist', 'dykes', 'fucking', 'niggas', 'white', 'supremacist', 'towelhead', 'attack', 'impunity', 'cancel', 'nuts', 'faggot', 'kabitch', 'nigger', 'black', 'whore', 'nigger', 'queers', 'christian', 'niggers', 'kill', 'freedom', 'fighter', 'yid']\n",
      "\n",
      "All words have been saved to 'all_words_list.txt'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def extract_all_words(file_path):\n",
    "    all_words = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Iterate through each entry and extract words\n",
    "    for entry in data:\n",
    "        words_str = entry.get('words', '')\n",
    "        # Split the words by comma\n",
    "        words = words_str.split(',')\n",
    "        # Strip any leading/trailing whitespace and add to the list\n",
    "        words = [word.strip() for word in words if word.strip()]\n",
    "        all_words.extend(words)\n",
    "\n",
    "    return all_words\n",
    "\n",
    "# Path to your JSON file\n",
    "json_file_path = 'misclassification_word_explanations.json'\n",
    "\n",
    "# Extract all words\n",
    "all_words = extract_all_words(json_file_path)\n",
    "\n",
    "# Optionally, to get unique words, uncomment the following line:\n",
    "# all_words = list(set(all_words))\n",
    "\n",
    "print(\"List of All Words:\")\n",
    "print(all_words)\n",
    "\n",
    "# If you want to save the list to a file, you can do so as follows:\n",
    "with open('all_words_list.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in all_words:\n",
    "        f.write(f\"{word}\\n\")\n",
    "\n",
    "print(f\"\\nAll words have been saved to 'all_words_list.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5093f961-17c6-4034-8c39-0ff3f43ae322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=\"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\")\n",
    "\n",
    "def read_words_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        words = file.read().splitlines()\n",
    "    return words\n",
    "\n",
    "def get_completion(prompt, model=\"llama3-8b-8192\", temperature=0.0, max_tokens=2000):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response\n",
    "\n",
    "file_path = 'all_words_list.txt'\n",
    "words = read_words_from_file(file_path)\n",
    "prompt = f\"\"\"Group the following words based on similarity. For each group, provide a descriptive label that accurately captures the common theme among the words. The groups should be as distinct as possible. After grouping, provide both the group label and the list of words for each group. Place all the words of dictionary in at least 1 group and if they don't fall in any group put them in others.\n",
    "\n",
    "Words:\n",
    "{', '.join(words)}\n",
    "\n",
    "Your output should be in this format:\n",
    "\n",
    "1. **[Race]**: word1, word2, word3, ...\n",
    "2. **[Religion]**: word1, word2, word3, ...\n",
    "3. **[Gender]**: word1, word2, word3, ...\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "# choices[0].message.content.strip()\n",
    "content = response.choices[0].message.content.strip()\n",
    "with open('grouped_words_output.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a49c8-5854-419d-b8cf-5d9350b65702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def zip_final_output(output_dir='final_output', output_filename='final_output.zip'):\n",
    "    \"\"\"\n",
    "    Compresses the specified output directory into a ZIP archive.\n",
    "\n",
    "    Parameters:\n",
    "    - output_dir (str): Path to the directory to compress.\n",
    "    - output_filename (str): Desired name for the ZIP file.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"Error: The directory '{output_dir}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Create a ZIP archive\n",
    "    shutil.make_archive(base_name=output_filename.replace('.zip', ''), format='zip', root_dir=output_dir)\n",
    "    print(f\"Successfully created '{output_filename}' containing the contents of '{output_dir}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    zip_final_output()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 2.0.0",
   "language": "python",
   "name": "torch200"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
