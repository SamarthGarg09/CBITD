{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deeksha/anaconda3/envs/myenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# optimizer, scheduler\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "# multi-label classification metrics\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#tqdm\n",
    "from tqdm.auto import tqdm\n",
    "#for ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>427918</td>\n",
       "      <td>can't understand your gibberish but it is tota...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5052426</td>\n",
       "      <td>\"moderates in Trump leaning states\" and Trump'...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5778344</td>\n",
       "      <td>The real Trump came out to play.\\n\\nWhile bein...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5729330</td>\n",
       "      <td>There is somewhere locked in your mind that fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7057187</td>\n",
       "      <td>Acosta was nearly a complete idiot.  Now he ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                       comment_text  toxicity  \\\n",
       "0   427918  can't understand your gibberish but it is tota...         1   \n",
       "1  5052426  \"moderates in Trump leaning states\" and Trump'...         0   \n",
       "2  5778344  The real Trump came out to play.\\n\\nWhile bein...         1   \n",
       "3  5729330  There is somewhere locked in your mind that fo...         0   \n",
       "4  7057187  Acosta was nearly a complete idiot.  Now he ha...         1   \n",
       "\n",
       "   severe_toxicity  obscene  sexual_explicit  identity_attack  insult  threat  \n",
       "0             0.00        1                0                0       1       0  \n",
       "1             0.00        0                0                0       0       0  \n",
       "2             0.00        1                0                1       1       0  \n",
       "3             0.00        0                0                0       0       0  \n",
       "4             0.05        1                0                1       1       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/train.csv')\n",
    "dev = pd.read_csv('dataset/dev.csv')\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7927</th>\n",
       "      <td>5718871</td>\n",
       "      <td>No mention of Fowler? I am expecting/hoping for good things from him this year. It's now or never for Benny.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17765</th>\n",
       "      <td>579523</td>\n",
       "      <td>stupid dog.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>7035560</td>\n",
       "      <td>Hillary is an unlikeable murderer. Trump is sticking it to middle class families and lied when he said he is in favor of lgbt rights. \\n\\nMy helath insurance is going up 30% due to trump. Obamacare looks amazing right now. Good job Trumpy?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17430</th>\n",
       "      <td>380235</td>\n",
       "      <td>Ward 1:  Without getting bogged down in blaming President Bush and his buddies for Iraq and Afghanistan or President Obama for continuing our involvement in the Middle East, may I wholeheartedly agree that we're failing our \"wounded warriors\" who have returned and are still returning from deployments in the MiddleEast.  Psychological wounds are destroying the lives of thousands of returning servicepersons.  For whatever reasons, the VA is not addressing the many issues the young men and women are facing.  They need help and support which they're not receiving.  We read of incidents when someone like Brian Babb dies......yet, we continue to fail to offer adequate support for  those with PTSD.  To me, and obviously as I read your posts, to you, this is a national tragedy.  \\n\\nI'd like to join you in working to get that support for the young men and women.  What can we do?  thanks and regards, Gary</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18751</th>\n",
       "      <td>833158</td>\n",
       "      <td>Exactly. I bet all of the people he listed drank water, too. Should we conclude that drinking water makes people violent? One of the first things you learn in a research methods class is that you can't infer causation from a correlation. And in this case, i'm not sure there is really a significant correlation. The plural of anecdote is not data.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  \\\n",
       "7927   5718871   \n",
       "17765   579523   \n",
       "57     7035560   \n",
       "17430   380235   \n",
       "18751   833158   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        comment_text  \\\n",
       "7927                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    No mention of Fowler? I am expecting/hoping for good things from him this year. It's now or never for Benny.   \n",
       "17765                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    stupid dog.   \n",
       "57                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Hillary is an unlikeable murderer. Trump is sticking it to middle class families and lied when he said he is in favor of lgbt rights. \\n\\nMy helath insurance is going up 30% due to trump. Obamacare looks amazing right now. Good job Trumpy?   \n",
       "17430  Ward 1:  Without getting bogged down in blaming President Bush and his buddies for Iraq and Afghanistan or President Obama for continuing our involvement in the Middle East, may I wholeheartedly agree that we're failing our \"wounded warriors\" who have returned and are still returning from deployments in the MiddleEast.  Psychological wounds are destroying the lives of thousands of returning servicepersons.  For whatever reasons, the VA is not addressing the many issues the young men and women are facing.  They need help and support which they're not receiving.  We read of incidents when someone like Brian Babb dies......yet, we continue to fail to offer adequate support for  those with PTSD.  To me, and obviously as I read your posts, to you, this is a national tragedy.  \\n\\nI'd like to join you in working to get that support for the young men and women.  What can we do?  thanks and regards, Gary   \n",
       "18751                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Exactly. I bet all of the people he listed drank water, too. Should we conclude that drinking water makes people violent? One of the first things you learn in a research methods class is that you can't infer causation from a correlation. And in this case, i'm not sure there is really a significant correlation. The plural of anecdote is not data.   \n",
       "\n",
       "       toxicity  severe_toxicity  obscene  sexual_explicit  identity_attack  \\\n",
       "7927          0         0.000000        0                0                0   \n",
       "17765         1         0.030769        1                0                0   \n",
       "57            1         0.043478        0                0                1   \n",
       "17430         0         0.000000        0                0                0   \n",
       "18751         0         0.000000        0                0                0   \n",
       "\n",
       "       insult  threat  \n",
       "7927        0       0  \n",
       "17765       1       0  \n",
       "57          1       0  \n",
       "17430       0       0  \n",
       "18751       0       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show full text\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36000, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['id', 'severe_toxicity', 'toxicity'], inplace=True)\n",
    "dev.drop(columns=['id', 'severe_toxicity', 'toxicity'], inplace=True)\n",
    "test.drop(columns=['id', 'severe_toxicity', 'toxicity'], inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['obscene', 'sexual_explicit', 'identity_attack', 'insult', 'threat'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = df.columns[1:]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "NUM_LABELS=5\n",
    "MAX_LENGTH=512\n",
    "BATCH_SIZE=64\n",
    "MODEL_NAME='saved_target_model'\n",
    "LR=2e-5\n",
    "NUM_WARMUP_STEPS=3600\n",
    "NUM_TRAIN_EPOCHS=20\n",
    "NUM_LOG_STEPS=2250\n",
    "NUM_SAVE_STEPS=2250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,  7424,    75,  1346,   110, 25863,  1943,  1173,    53,    24,\n",
       "            16,   746, 20175,     4,  1437,  1437,   114,    79,    34,   143,\n",
       "          2956,    59,   878,    13,   558,    11,  6467,     6,  4309,    24,\n",
       "             6,    79,    16, 20260,     4,  1437,    42,  1160,    40, 28297,\n",
       "            69,  6000,     4,  1437,    79,   429,    25,   157,   517,   124,\n",
       "             7, 45968,   281,    50,   277,   194,     4,  1437,    82,   218,\n",
       "            75,  4309, 27338,  2258,     4,     2,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([1., 0., 0., 1., 0.])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multi-label dataset\n",
    "class ToxicityDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.labels = df[['obscene', 'threat', 'sexual_explicit', 'insult', 'identity_attack']].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        comment = self.df.iloc[idx].comment_text\n",
    "        inputs = self.tokenizer(\n",
    "            comment,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "model_name = 'roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dataset = ToxicityDataset(df, tokenizer, 128)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ToxicityDataset(df, tokenizer, MAX_LENGTH)\n",
    "dev_dataset = ToxicityDataset(dev, tokenizer, MAX_LENGTH)\n",
    "test_dataset = ToxicityDataset(test, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "563"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at saved_target_model and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model, optimizer, scheduler, loss_fn\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"saved_target_model\", num_labels=NUM_LABELS, problem_type=\"multi_label_classification\", ignore_mismatched_sizes=True)\n",
    "model.to('cuda')\n",
    "target_layer = 'roberta.encoder.layer.10'\n",
    "\n",
    "unfreeze = False\n",
    "for name, layer in model.named_modules():\n",
    "    if name == target_layer:\n",
    "        unfreeze = True\n",
    "    if not unfreeze:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True  \n",
    "            \n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = AdamW(filter(lambda p:p.requires_grad, model.parameters()), lr=LR)\n",
    "NUM_TRAIN_STEPS=len(train_loader)*NUM_TRAIN_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=NUM_WARMUP_STEPS, num_training_steps=NUM_TRAIN_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11260"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_STEPS = len(train_loader) * NUM_TRAIN_EPOCHS\n",
    "TOTAL_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11260 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 86/11260 [01:49<3:57:18,  1.27s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Append the current training loss\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m loss_i\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Evaluation and logging every NUM_LOG_STEPS\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m NUM_LOG_STEPS \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Switch to evaluation mode\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "TOTAL_STEPS = len(train_loader) * NUM_TRAIN_EPOCHS\n",
    "loss_i = []\n",
    "val_loss, val_acc = [], []\n",
    "saved_models = []\n",
    "step = 0\n",
    "train_iter = iter(train_loader)\n",
    "\n",
    "for step in tqdm(range(TOTAL_STEPS)):\n",
    "    try:\n",
    "        batch = next(train_iter)\n",
    "    except StopIteration:\n",
    "        train_iter = iter(train_loader)\n",
    "        batch = next(train_iter)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "    \n",
    "    logits = model(batch['input_ids'], attention_mask=batch['attention_mask']).logits\n",
    "    loss = criterion(logits, batch['labels'])\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    loss_i.append(loss.item())\n",
    "    \n",
    "    if step % NUM_LOG_STEPS == 0 and step > 0:\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        correct_predictions = np.zeros(len(cols))\n",
    "        total_predictions = np.zeros(len(cols))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dev_loader, leave=False, total=len(dev_loader)):\n",
    "                batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "                logits = model(batch['input_ids'], attention_mask=batch['attention_mask']).logits\n",
    "                loss = criterion(logits, batch['labels'])\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.sigmoid(logits).cpu().numpy()\n",
    "                labels = batch['labels'].cpu().numpy()\n",
    "                \n",
    "                preds = (preds > 0.5)\n",
    "                correct_predictions += np.sum(preds == labels, axis=0)\n",
    "                total_predictions += labels.shape[0]\n",
    "        \n",
    "        val_loss_epoch = total_val_loss / len(dev_loader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "        val_loss.append(val_loss_epoch)\n",
    "        val_acc.append(accuracy)\n",
    "        \n",
    "        print(f\"Step {step} | Training Loss: {loss_i[-1]:.4f} | Validation Loss: {val_loss_epoch:.4f} | Validation Accuracy: {accuracy}\")\n",
    "        \n",
    "        model.train()\n",
    "    \n",
    "    if step % NUM_SAVE_STEPS == 0 and step > 0:\n",
    "        if len(saved_models) < 2 or val_loss_epoch < max(saved_models, key=lambda x: x[1])[1]:\n",
    "            model_save_path = f'model_checkpoint_step_{step}.pt'\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            saved_models.append((model_save_path, val_loss_epoch))\n",
    "            saved_models = sorted(saved_models, key=lambda x: x[1])[:2]\n",
    "            \n",
    "            print(f\"Model checkpoint saved at {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 186/1250 [00:20<01:59,  8.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], attention_mask\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 14\u001b[0m test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(logits)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     16\u001b[0m preds \u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "from collections import defaultdict\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct_predictions = np.zeros(len(cols))  \n",
    "total_predictions=np.zeros(len(cols))\n",
    "samples = defaultdict(list)\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        batch = {k:v.to('cuda') for k,v in batch.items()}\n",
    "        logits = model(batch['input_ids'], attention_mask=batch['attention_mask']).logits\n",
    "        loss = criterion(logits, batch['labels'])\n",
    "        test_loss += loss.item()\n",
    "        preds = torch.sigmoid(logits).cpu().numpy()\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        preds = (preds > 0.5)\n",
    "        \n",
    "        correct_predictions += (preds==labels).sum(axis=0)\n",
    "        total_predictions += labels.shape[0]\n",
    "        \n",
    "        decoded_text = tokenizer.batch_decode(batch['input_ids'], skip_special_tokens=True)\n",
    "        samples['text'].extend(decoded_text)\n",
    "        samples['labels'].extend(labels)\n",
    "        samples['preds'].extend(preds)\n",
    "        \n",
    "    total_loss = test_loss / len(test_loader)\n",
    "    total_accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    print(f\"Test Loss: {total_loss:.4f} | Test Accuracy: {total_accuracy}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_100 = random.sample(list(zip(samples['text'], samples['labels'], samples['preds'])), 100)\n",
    "\n",
    "dump = pd.DataFrame(random_100, columns=['text', 'labels', 'preds'])\n",
    "dump.to_csv('random_100.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>I believe in climate change, always have. However I'm more concerned about the potential for a new ice age (which also has growing scientific support) than I am about warming trends. The reality is according to many climate change models, when it comes to a warming climate Canada is (will be) a net beneficiary (agriculture Canada has a good summary). So rather than \"fighting\" global warming we should be encouraging it.</td>\n",
       "      <td>[0. 0. 0. 0. 0.]</td>\n",
       "      <td>[False False False False False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Is there any writers at ADN that will report honestly and fairly to both sides of an issue?</td>\n",
       "      <td>[0. 0. 0. 0. 0.]</td>\n",
       "      <td>[False False False False False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>When will the media have the courage to interview the parents who brought their kids here illegally and ask why they never tried to gain legal citizenship?\\n\\nInstead they always blame the laws on the books. These kids should go home and talk to their parents and not walk out in a protest. Hopefully we do not cave on this issue.</td>\n",
       "      <td>[0. 0. 0. 0. 0.]</td>\n",
       "      <td>[False False False False False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Yeah, who doesn't want more of this.\\nBC Hydro’s real debt has grown 1337% under Liberals\\nhttp://commonsensecanadian.ca/bc-hydro-real-debt-grown-1337-liberals-shouldnt-someone-call-cops/\\n\\n98 BC Liberal Falsehoods, Boondoggles and Scandals: The Clark Era 53\\nhttps://thetyee.ca/Opinion/2017/04/04/BC-Liberal-Falsehoods-Scandals-Clark/</td>\n",
       "      <td>[0. 0. 0. 0. 0.]</td>\n",
       "      <td>[False False False False False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>A team of home-schooled children from Wasilla out tested the entire rest of the state and will be representing Alaska at the 2016 National Academic Decathlon.\\n\\nAlaska is second in the nation with per capita education spending of $18,175 per student, while simultaneously holding the shameful distinction of having 63% of 8th grade students not proficient in either reading or math. (source NAEP)\\n\\nThese big government money wasters should get their own house in order, before pointing fingers at home-schoolers.</td>\n",
       "      <td>[0. 0. 0. 0. 0.]</td>\n",
       "      <td>[False False False False False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>good idea!\\npoop on the nay-sayers until their kids, or mother are the one hit or killed, then they'll change their minds.</td>\n",
       "      <td>[1. 1. 0. 1. 1.]</td>\n",
       "      <td>[False False False False False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Businesses should not be taxed anything more than municipal property tax. If the Libs had any tact they'd be making business-friendly moves to compete with USA. They'd also be managing HST and income tax revenues responsibly.</td>\n",
       "      <td>[0. 0. 0. 0. 0.]</td>\n",
       "      <td>[False False False False False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sure, then let's have a bloated, egotistical, trash talking, racist carry the nuclear codes.\\nPerhaps you need a little more sleep?</td>\n",
       "      <td>[1. 1. 0. 1. 1.]</td>\n",
       "      <td>[ True False False  True  True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>What an utter buffoon.</td>\n",
       "      <td>[1. 0. 0. 1. 0.]</td>\n",
       "      <td>[ True False False  True  True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Wallonia does not suppress Flemish language the way Quebec suppresses English language</td>\n",
       "      <td>[0. 0. 0. 0. 0.]</td>\n",
       "      <td>[False False False False False]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text  \\\n",
       "20                                                                                               I believe in climate change, always have. However I'm more concerned about the potential for a new ice age (which also has growing scientific support) than I am about warming trends. The reality is according to many climate change models, when it comes to a warming climate Canada is (will be) a net beneficiary (agriculture Canada has a good summary). So rather than \"fighting\" global warming we should be encouraging it.   \n",
       "51                                                                                                                                                                                                                                                                                                                                                                                                                                          Is there any writers at ADN that will report honestly and fairly to both sides of an issue?   \n",
       "15                                                                                                                                                                                           When will the media have the courage to interview the parents who brought their kids here illegally and ask why they never tried to gain legal citizenship?\\n\\nInstead they always blame the laws on the books. These kids should go home and talk to their parents and not walk out in a protest. Hopefully we do not cave on this issue.   \n",
       "95                                                                                                                                                                                     Yeah, who doesn't want more of this.\\nBC Hydro’s real debt has grown 1337% under Liberals\\nhttp://commonsensecanadian.ca/bc-hydro-real-debt-grown-1337-liberals-shouldnt-someone-call-cops/\\n\\n98 BC Liberal Falsehoods, Boondoggles and Scandals: The Clark Era 53\\nhttps://thetyee.ca/Opinion/2017/04/04/BC-Liberal-Falsehoods-Scandals-Clark/   \n",
       "65  A team of home-schooled children from Wasilla out tested the entire rest of the state and will be representing Alaska at the 2016 National Academic Decathlon.\\n\\nAlaska is second in the nation with per capita education spending of $18,175 per student, while simultaneously holding the shameful distinction of having 63% of 8th grade students not proficient in either reading or math. (source NAEP)\\n\\nThese big government money wasters should get their own house in order, before pointing fingers at home-schoolers.   \n",
       "43                                                                                                                                                                                                                                                                                                                                                                                                           good idea!\\npoop on the nay-sayers until their kids, or mother are the one hit or killed, then they'll change their minds.   \n",
       "72                                                                                                                                                                                                                                                                                                    Businesses should not be taxed anything more than municipal property tax. If the Libs had any tact they'd be making business-friendly moves to compete with USA. They'd also be managing HST and income tax revenues responsibly.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                   Sure, then let's have a bloated, egotistical, trash talking, racist carry the nuclear codes.\\nPerhaps you need a little more sleep?   \n",
       "83                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               What an utter buffoon.   \n",
       "35                                                                                                                                                                                                                                                                                                                                                                                                                                               Wallonia does not suppress Flemish language the way Quebec suppresses English language   \n",
       "\n",
       "              labels                            preds  \n",
       "20  [0. 0. 0. 0. 0.]  [False False False False False]  \n",
       "51  [0. 0. 0. 0. 0.]  [False False False False False]  \n",
       "15  [0. 0. 0. 0. 0.]  [False False False False False]  \n",
       "95  [0. 0. 0. 0. 0.]  [False False False False False]  \n",
       "65  [0. 0. 0. 0. 0.]  [False False False False False]  \n",
       "43  [1. 1. 0. 1. 1.]  [False False False False False]  \n",
       "72  [0. 0. 0. 0. 0.]  [False False False False False]  \n",
       "9   [1. 1. 0. 1. 1.]  [ True False False  True  True]  \n",
       "83  [1. 0. 0. 1. 0.]  [ True False False  True  True]  \n",
       "35  [0. 0. 0. 0. 0.]  [False False False False False]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "dump = pd.read_csv('random_100.csv')\n",
    "dump.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8., 7., 7.], dtype=torch.float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0], [1, 0, 1], [0, 1, 0], [1, 0, 1], [0, 1, 0], [1, 0, 1]])\n",
    "preds = np.array([[0, 0, 1], [1, 0, 1], [0, 1, 0], [1, 0, 1], [0, 1, 0], [1, 0, 1], [0, 1, 0], [1, 0, 1]])\n",
    "a = torch.zeros(3)\n",
    "a += np.sum(preds == labels, axis=0)\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
