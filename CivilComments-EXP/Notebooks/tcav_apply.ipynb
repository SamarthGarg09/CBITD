{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de50c3fe-90ed-4a35-840f-06f8c8de4537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tcav import TCAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "940a78fb-5ea5-4c76-bc39-4c2956087f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcav import TCAV\n",
    "from torch import nn\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea4ddc04-6897-44c4-8618-a57e2da870c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class X2YModel(nn.Module):\n",
    "    def __init__(self, model_name='saved_target_model', num_classes=2):\n",
    "        super(X2YModel, self).__init__()\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, inputs_embeds=None):\n",
    "        if inputs_embeds is not None:\n",
    "            outputs = self.model.roberta(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        else:\n",
    "            input_ids = input_ids.long()\n",
    "            with torch.no_grad():\n",
    "                input_embeds = self.model.roberta.get_input_embeddings()(input_ids)\n",
    "            # print(input_embeds.shape)\n",
    "            outputs = self.model.roberta(inputs_embeds=input_embeds, attention_mask=attention_mask)\n",
    "        \n",
    "        return self.model.classifier(outputs.last_hidden_state)\n",
    "\n",
    "x2y_model = X2YModel().to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0312ca5d-375a-41e5-8277-e9de8279faa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/2017025/sgarg01/.python3-3.12-torch220/site-packages/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec9d540ee134d738b292d589818e1d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee3b0bff49d4049b35f038b3ed30b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/train.csv')\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"comment_text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "def prepare_labels(examples):\n",
    "\n",
    "    label_columns = [\"obscene\", \"threat\", \"insult\", \"identity_attack\", \"sexual_explicit\"]\n",
    "    labels = []\n",
    "    for i in range(len(examples[label_columns[0]])):\n",
    "        labels.append([float(examples[column][i]) for column in label_columns])\n",
    "    return {\"labels\": labels}\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(prepare_labels, batched=True)\n",
    "\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3108db7a-bc43-4da9-8632-b6f191f681a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading random_CAVs.npz, CAVs.npz and metrics.npz from cache\n",
      "Using cached layer names: ['model.roberta.encoder.layer.11.output.dense']\n"
     ]
    }
   ],
   "source": [
    "layer_names = ['model.roberta.encoder.layer.11.output.dense']# #'model.roberta.encoder.layer.10.output.dense', 'model.roberta.encoder.layer.9.output.dense']\n",
    "\n",
    "def calculate_pos_weight(dl):\n",
    "    pos_cnt = None\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dl:\n",
    "            if pos_cnt is None:\n",
    "                # print(batch['labels'])\n",
    "                pos_cnt = batch['labels'].sum(0).clone()\n",
    "            else:\n",
    "                pos_cnt += batch['labels'].sum(0).clone()\n",
    "            cnt += batch['labels'].shape[0]\n",
    "    \n",
    "    neg_cnt = cnt - pos_cnt\n",
    "    pos_weight = neg_cnt / pos_cnt\n",
    "    \n",
    "    return pos_weight\n",
    "\n",
    "tcav = TCAV(x2y_model, layer_names=layer_names, cache_dir='cav')\n",
    "hparams = dict(task='classification', n_epochs=10, patience=10, batch_size=1, lr=1e-3, weight_decay=1e-2, pos_weight=None)\n",
    "train_dl = DataLoader(train_dataset, batch_size=hparams['batch_size'], shuffle=False)\n",
    "test_dl = DataLoader(test_dataset, batch_size=hparams['batch_size'], shuffle=False)\n",
    "pos_weights = calculate_pos_weight(train_dl)\n",
    "hparams['pos_weight'] = pos_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9081fd6e-53b9-49ef-b16c-1edac3ac3841",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tcav.generate_random_CAVs(train_dataset, test_dataset, n_repeat=1, force_rewrite_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d404e-6dc0-4148-8fe4-b044980b50eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating TCAV for layers: ['model.roberta.encoder.layer.11.output.dense']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fedbaf0ff84a4d11988cf7ded345007e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#repeats:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d5394278dd4b0bbb8a3dfbdd5f16a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Layers:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear Model for layer: model.roberta.encoder.layer.11.output.dense\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9b1442d0724a2d93cdd5b25497e3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tcav.generate_CAVs(train_dataset, test_dataset, n_repeats=1, hparams=hparams, force_rewrite_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9713def-1248-477e-990c-8432793c1004",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tcav.generate_TCAVs(test_dataset, 'model.roberta.encoder.layer.11.output.dense', target_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d41fb14-2b54-4706-9156-3ebf58912f25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/2017025/sgarg01/.python3-3.12-torch220/site-packages/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5b3559324740b6b669013a383fa61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/232 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num concepts: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd4c929f61b48b189c5bfa6a9dd920b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/232 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from datasets import Dataset\n",
    "\n",
    "df_test = pd.read_csv(\"analysis_sheets/final_misclassified_samples_with_concept_gradients.csv\")\n",
    "ds_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = ds_test.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x2y_dl_test = DataLoader(tokenized_dataset, batch_size=1, shuffle=False)\n",
    "n_concepts = 5\n",
    "print(f'Num concepts: {n_concepts}')\n",
    "# tcav = TCAV(x2y_model, layer_names=['model.roberta.encoder.layer.11.output.dense'], cache_dir='cav')\n",
    "\n",
    "attrs = []\n",
    "\n",
    "for batch in tqdm(x2y_dl_test, leave=True):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = x2y_model.model.get_input_embeddings()(input_ids)\n",
    "        # embeddings = x2c_model.model.get_input_embeddings()(input_ids)\n",
    "        \n",
    "    embeddings.requires_grad_(True)\n",
    "    attention_mask = attention_mask.float()\n",
    "    attention_mask.requires_grad_(True)\n",
    "    target = batch['label'].to(device)\n",
    "    print(target)\n",
    "    # attr = tcav.attribute((None, attention_mask, embeddings), 'model.roberta.encoder.layer.11.output.dense', 'cosine_similarity', target=y)\n",
    "    attr = tcav.attribute(\n",
    "        layer_name='model.roberta.encoder.layer.11.output.dense',\n",
    "        mode='cosine_similarity',\n",
    "        target=target.item(),\n",
    "        use_random=False,\n",
    "        input_ids=None,\n",
    "        attention_mask=attention_mask,\n",
    "        inputs_embeds=embeddings\n",
    "    )\n",
    "    attr = attr.detach().cpu().numpy()\n",
    "    attrs.append(attr)\n",
    "\n",
    "attrs = np.concatenate(attrs, axis=0)\n",
    "\n",
    "np.save('cav/attr_npy_classification.npy', attrs)\n",
    "\n",
    "\n",
    "attrs = np.load('cav/attr_npy_classification.npy')\n",
    "df = df_test.copy()\n",
    "df['tcav_score2'] = attrs.tolist()\n",
    "# reduce the precision of the tcav scores\n",
    "df.tcav_score2 = df.tcav_score2.apply(lambda x: [round(i, 8) for i in x])\n",
    "# df.tcav_score[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d04723ba-63ee-4828-8750-f36ea667e4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>target_logits</th>\n",
       "      <th>concept_logits</th>\n",
       "      <th>concept_gradient</th>\n",
       "      <th>label</th>\n",
       "      <th>concept_label</th>\n",
       "      <th>tcav_score2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>Darkness! \\n\\nTerrible loss.  Hopefully his is kicking the carp out of Rick James in heaven.  What did the five fingers say top the face?  SLAP!</td>\n",
       "      <td>[[0.23067841 0.76932156]]</td>\n",
       "      <td>[[0.5351278  0.24382626 0.33698714 0.7738781  0.7711287 ]]</td>\n",
       "      <td>[[-0.0293295  -0.01354797 -0.01953403 -0.03894239 -0.01446827]]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0 0 0 0 0]]</td>\n",
       "      <td>[-0.06967939, -0.03928981, -0.12477093, -0.06573925, -0.04007399]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>If he is an honorable man, he would resign and let the public know why.  I feel NO sorrow for those working for Trumplethinskin and, when they find out how used they are, still keep the jobs and pout in the corner.  Honor can come with a price.  And the idea that they can stick around and hope to have a positive effect on a narcissistic sociopath is sadly laughable.</td>\n",
       "      <td>[[0.51957065 0.48042935]]</td>\n",
       "      <td>[[0.12699111 0.02351919 0.08125411 0.9052289  0.02437039]]</td>\n",
       "      <td>[[0.05448355 0.04270548 0.02316145 0.05714341 0.04204933]]</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0 0 0 1 1]]</td>\n",
       "      <td>[0.0497874, 0.03345869, 0.1465009, 0.04943258, 0.02268603]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>It's too bad that he has to play for the Panthers and play with a whiny Cam Newton.</td>\n",
       "      <td>[[0.993095   0.00690502]]</td>\n",
       "      <td>[[0.08773369 0.03604625 0.13983132 0.5414997  0.02529005]]</td>\n",
       "      <td>[[0.06258741 0.01472565 0.0204106  0.09105707 0.04605943]]</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0 0 0 1 0]]</td>\n",
       "      <td>[0.04561663, 0.02056997, 0.14326172, 0.04327109, 0.03282209]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>Criminals like Comey, Clinton, Lynch &amp; Obama are supposed to be PUNISHED!</td>\n",
       "      <td>[[0.72191566 0.27808437]]</td>\n",
       "      <td>[[0.1714845  0.06448447 0.15295745 0.6764379  0.31749308]]</td>\n",
       "      <td>[[0.04101431 0.01374963 0.0143908  0.05260455 0.01483291]]</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0 1 0 1 0]]</td>\n",
       "      <td>[0.04518934, 0.02845072, 0.15218557, 0.05489176, 0.02115705]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>money is much more important than children.  no matter how many children are destroyed, faithful Catholics will always make more.</td>\n",
       "      <td>[[0.981703   0.01829699]]</td>\n",
       "      <td>[[0.1209175  0.2900798  0.7965846  0.58739597 0.27424332]]</td>\n",
       "      <td>[[ 0.06256711 -0.00696945 -0.01156354  0.08550084  0.00439526]]</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0 1 0 1 1]]</td>\n",
       "      <td>[0.04743417, 0.03753437, 0.11939464, 0.02219711, 0.03659673]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  \\\n",
       "93          93   \n",
       "28          28   \n",
       "55          55   \n",
       "30          30   \n",
       "80          80   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                            sentence  \\\n",
       "93                                                                                                                                                                                                                                  Darkness! \\n\\nTerrible loss.  Hopefully his is kicking the carp out of Rick James in heaven.  What did the five fingers say top the face?  SLAP!   \n",
       "28  If he is an honorable man, he would resign and let the public know why.  I feel NO sorrow for those working for Trumplethinskin and, when they find out how used they are, still keep the jobs and pout in the corner.  Honor can come with a price.  And the idea that they can stick around and hope to have a positive effect on a narcissistic sociopath is sadly laughable.   \n",
       "55                                                                                                                                                                                                                                                                                               It's too bad that he has to play for the Panthers and play with a whiny Cam Newton.   \n",
       "30                                                                                                                                                                                                                                                                                                         Criminals like Comey, Clinton, Lynch & Obama are supposed to be PUNISHED!   \n",
       "80                                                                                                                                                                                                                                                 money is much more important than children.  no matter how many children are destroyed, faithful Catholics will always make more.   \n",
       "\n",
       "                target_logits  \\\n",
       "93  [[0.23067841 0.76932156]]   \n",
       "28  [[0.51957065 0.48042935]]   \n",
       "55  [[0.993095   0.00690502]]   \n",
       "30  [[0.72191566 0.27808437]]   \n",
       "80  [[0.981703   0.01829699]]   \n",
       "\n",
       "                                                concept_logits  \\\n",
       "93  [[0.5351278  0.24382626 0.33698714 0.7738781  0.7711287 ]]   \n",
       "28  [[0.12699111 0.02351919 0.08125411 0.9052289  0.02437039]]   \n",
       "55  [[0.08773369 0.03604625 0.13983132 0.5414997  0.02529005]]   \n",
       "30  [[0.1714845  0.06448447 0.15295745 0.6764379  0.31749308]]   \n",
       "80  [[0.1209175  0.2900798  0.7965846  0.58739597 0.27424332]]   \n",
       "\n",
       "                                                   concept_gradient  label  \\\n",
       "93  [[-0.0293295  -0.01354797 -0.01953403 -0.03894239 -0.01446827]]      0   \n",
       "28       [[0.05448355 0.04270548 0.02316145 0.05714341 0.04204933]]      1   \n",
       "55       [[0.06258741 0.01472565 0.0204106  0.09105707 0.04605943]]      1   \n",
       "30       [[0.04101431 0.01374963 0.0143908  0.05260455 0.01483291]]      1   \n",
       "80  [[ 0.06256711 -0.00696945 -0.01156354  0.08550084  0.00439526]]      1   \n",
       "\n",
       "    concept_label  \\\n",
       "93  [[0 0 0 0 0]]   \n",
       "28  [[0 0 0 1 1]]   \n",
       "55  [[0 0 0 1 0]]   \n",
       "30  [[0 1 0 1 0]]   \n",
       "80  [[0 1 0 1 1]]   \n",
       "\n",
       "                                                          tcav_score2  \n",
       "93  [-0.06967939, -0.03928981, -0.12477093, -0.06573925, -0.04007399]  \n",
       "28         [0.0497874, 0.03345869, 0.1465009, 0.04943258, 0.02268603]  \n",
       "55       [0.04561663, 0.02056997, 0.14326172, 0.04327109, 0.03282209]  \n",
       "30       [0.04518934, 0.02845072, 0.15218557, 0.05489176, 0.02115705]  \n",
       "80       [0.04743417, 0.03753437, 0.11939464, 0.02219711, 0.03659673]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "077143dc-f9cf-4cf8-bdea-98990e7bf0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(columns=['tcav_score'], inplace=True)\n",
    "df.to_csv('analysis_sheets/final_misclassified_samples_with_concept_gradients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22e0e57d-e7e2-4c9b-8a1d-b73754fb9b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(232, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f124ebcb-171e-4edd-a003-7aa304af9fff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tcav_score2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>“personal attacks” on judges who blocked the administration’s first travel ban were “out of all bounds of civic and persuasive discourse.”\\n\\nWhy on earth would that be.  Every decision maker is subject to personal criticism for their decision.  Do the judges believe that they are oracles from God?</td>\n",
       "      <td>[-0.08191304, -0.07248733, -0.12390038, -0.08451594, -0.05713203]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>Oh the half orangutan is threatening to clean house alright.  He says that if he can't do a White House makeover he's going to live in his bankrupt Trump penthouse.  Keep 'em all in the penthouse.  Good riddance.</td>\n",
       "      <td>[0.07920141, 0.05130164, 0.20633397, 0.12203404, 0.0499733]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>I now hate Canada. \\n\\nI already hated Justin Turdeau</td>\n",
       "      <td>[0.0743716, 0.04591805, 0.18898252, 0.11009905, 0.04476108]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>You have history upside down and inside out. Are you talking about the Missiles to Megawatts program where the US bought uranium FROM Russia to power OUR country? \\n\\nhttps://en.wikipedia.org/wiki/Megatons_to_Megawatts_Program\\n\\nThat program was widely praised as a swords-to-plowshares type of initiative that was beneficial and helped reduce the aging nuclear stockpile. \\n\\nIt's as if your facts got put in a blender and you typed what poured out.</td>\n",
       "      <td>[-0.06722722, -0.05753541, -0.09889112, -0.06018158, -0.04677019]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>OMG a new couple to have drinkies in Ottawa.  They will be on the top of all the wannabe's \"dinner with\" lists.  Hopefully they won't be here long and have to pay for their basketball tickets..... BWWAAA</td>\n",
       "      <td>[-0.08143159, -0.07067852, -0.12199756, -0.07815488, -0.05584906]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                sentence  \\\n",
       "812                                                                                                                                                          “personal attacks” on judges who blocked the administration’s first travel ban were “out of all bounds of civic and persuasive discourse.”\\n\\nWhy on earth would that be.  Every decision maker is subject to personal criticism for their decision.  Do the judges believe that they are oracles from God?   \n",
       "916                                                                                                                                                                                                                                                 Oh the half orangutan is threatening to clean house alright.  He says that if he can't do a White House makeover he's going to live in his bankrupt Trump penthouse.  Keep 'em all in the penthouse.  Good riddance.   \n",
       "339                                                                                                                                                                                                                                                                                                                                                                                                                I now hate Canada. \\n\\nI already hated Justin Turdeau   \n",
       "192  You have history upside down and inside out. Are you talking about the Missiles to Megawatts program where the US bought uranium FROM Russia to power OUR country? \\n\\nhttps://en.wikipedia.org/wiki/Megatons_to_Megawatts_Program\\n\\nThat program was widely praised as a swords-to-plowshares type of initiative that was beneficial and helped reduce the aging nuclear stockpile. \\n\\nIt's as if your facts got put in a blender and you typed what poured out.   \n",
       "203                                                                                                                                                                                                                                                          OMG a new couple to have drinkies in Ottawa.  They will be on the top of all the wannabe's \"dinner with\" lists.  Hopefully they won't be here long and have to pay for their basketball tickets..... BWWAAA   \n",
       "\n",
       "                                                           tcav_score2  label  \n",
       "812  [-0.08191304, -0.07248733, -0.12390038, -0.08451594, -0.05713203]      0  \n",
       "916        [0.07920141, 0.05130164, 0.20633397, 0.12203404, 0.0499733]      1  \n",
       "339        [0.0743716, 0.04591805, 0.18898252, 0.11009905, 0.04476108]      1  \n",
       "192  [-0.06722722, -0.05753541, -0.09889112, -0.06018158, -0.04677019]      0  \n",
       "203  [-0.08143159, -0.07067852, -0.12199756, -0.07815488, -0.05584906]      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df = pd.read_csv('analysis_sheets/final_classified_samples_with_concept_gradients.csv')\n",
    "df.sample(5)[['sentence', 'tcav_score2', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7f5e7b-52a9-41ce-a168-5305c302180b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from scipy.stats import ttest_ind\n",
    "from captum.attr import LayerActivation\n",
    "from captum._utils.gradient import compute_layer_gradients_and_eval\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CelebA, ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchmetrics\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm, trange\n",
    "import PIL\n",
    "import seaborn\n",
    "import random\n",
    "import argparse\n",
    "import torch.cuda.amp as amp\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tcav import TCAV\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from datasets import Dataset\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class X2YModel(nn.Module):\n",
    "    def __init__(self, model_name='saved_target_model', num_classes=2):\n",
    "        super(X2YModel, self).__init__()\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, inputs_embeds=None):\n",
    "        if inputs_embeds is not None:\n",
    "            outputs = self.model.roberta(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        else:\n",
    "            input_ids = input_ids.long()\n",
    "            outputs = self.model.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        return self.model.classifier(outputs.last_hidden_state)\n",
    "\n",
    "x2y_model = X2YModel().to(device)\n",
    "\n",
    "df = pd.read_csv('dataset/train.csv')\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"comment_text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "def prepare_labels(examples):\n",
    "    label_columns = [\"obscene\", \"threat\", \"insult\", \"identity_attack\", \"sexual_explicit\"]\n",
    "    labels = []\n",
    "    for i in range(len(examples[label_columns[0]])):\n",
    "        labels.append([float(examples[column][i]) for column in label_columns])\n",
    "    return {\"labels\": labels}\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(prepare_labels, batched=True)\n",
    "\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "layer_names = ['model.roberta.encoder.layer.11.output.dense', ]#'model.roberta.encoder.layer.10.output.dense', 'model.roberta.encoder.layer.9.output.dense']\n",
    "\n",
    "def calculate_pos_weight(dl):\n",
    "    pos_cnt = None\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dl:\n",
    "            if pos_cnt is None:\n",
    "                pos_cnt = batch['labels'].sum(0).clone()\n",
    "            else:\n",
    "                pos_cnt += batch['labels'].sum(0).clone()\n",
    "            cnt += batch['labels'].shape[0]\n",
    "    \n",
    "    neg_cnt = cnt - pos_cnt\n",
    "    pos_weight = neg_cnt / pos_cnt\n",
    "    \n",
    "    return pos_weight\n",
    "\n",
    "tcav = TCAV(x2y_model, layer_names=layer_names, cache_dir='cav')\n",
    "\n",
    "hparams = dict(task='classification', n_epochs=10, patience=10, batch_size=32, lr=1e-3, weight_decay=1e-2, pos_weight=None)\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=hparams['batch_size'], shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size=hparams['batch_size'], shuffle=False)\n",
    "pos_weights = calculate_pos_weight(train_dl)\n",
    "hparams['pos_weight'] = pos_weights\n",
    "\n",
    "# tcav.generate_CAVs(train_dataset, test_dataset, n_repeats=2, hparams=hparams, force_rewrite_cache=True)\n",
    "\n",
    "df_test = pd.read_csv(\"analysis_sheets/final_misclassified_samples_with_concept_gradients.csv\")\n",
    "ds_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = ds_test.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "x2y_dl_test = DataLoader(tokenized_dataset, batch_size=8, shuffle=False)\n",
    "n_concepts = 5\n",
    "print(f'Num concepts: {n_concepts}')\n",
    "\n",
    "# tcav = TCAV(x2y_model, layer_names=['model.roberta.encoder.layer.11.output.dense'], cache_dir='cav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04962ca2-9995-4e6b-aeb6-c86a02f5654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = []\n",
    "\n",
    "for batch in tqdm(x2y_dl_test, leave=True):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    y = batch['label'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = x2y_model.model.get_input_embeddings()(input_ids)\n",
    "\n",
    "    embeddings.requires_grad_(True)\n",
    "    print(embeddings.shape)\n",
    "    attr = tcav.attribute(\n",
    "        layer_name='model.roberta.encoder.layer.11.output.dense',\n",
    "        mode='cosine_similarity',\n",
    "        target=y,\n",
    "        attention_mask=attention_mask,\n",
    "        inputs_embeds=embeddings\n",
    "    )\n",
    "    attr = attr.detach().cpu().numpy()\n",
    "    attrs.append(attr)\n",
    "\n",
    "attrs = np.concatenate(attrs, axis=0)\n",
    "\n",
    "np.save('cav/attr_npy_classification.npy', attrs)\n",
    "\n",
    "# Process and save the results\n",
    "attrs = np.load('cav/attr_npy_classification.npy')\n",
    "df = df_test.copy()\n",
    "df['tcav_score2'] = attrs.tolist()\n",
    "# Reduce the precision of the tcav scores\n",
    "df['tcav_score2'] = df['tcav_score2'].apply(lambda x: [round(float(i), 8) for i in x])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 2.2.0",
   "language": "python",
   "name": "torch220"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
