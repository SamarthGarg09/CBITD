{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27a64372-8d79-4ed7-ab2f-ec9376f99116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from concept_gradient import ConceptGradients\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f00e0528-305f-4ff0-a192-f03cddbe09fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dfc682cc-79d2-470c-b70c-523a62b94b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class X2YModel(nn.Module):\n",
    "    def __init__(self, model_name='./saved_target_model', num_classes=2):\n",
    "        super(X2YModel, self).__init__()\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, inputs_embeds=None):\n",
    "        if inputs_embeds is not None:\n",
    "            outputs = self.model.roberta(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        else:\n",
    "            outputs = self.model.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        return self.model.classifier(outputs.last_hidden_state)  \n",
    "\n",
    "class X2CModel(nn.Module):\n",
    "    def __init__(self, model_name='./saved_concept_model', num_concepts=5):\n",
    "        super(X2CModel, self).__init__()\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_concepts, ignore_mismatched_sizes=True).to('cuda')\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, inputs_embeds=None):\n",
    "        if inputs_embeds is not None:\n",
    "            outputs = self.model.roberta(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        else:\n",
    "            outputs = self.model.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        return self.model.classifier(outputs.last_hidden_state)\n",
    "\n",
    "x2y_model = X2YModel().to(device)\n",
    "x2c_model = X2CModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "588419a3-3728-47d8-841f-d2135d9fbfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_func(embeddings, attention_mask):\n",
    "    output = x2y_model(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
    "    return output\n",
    "\n",
    "def concept_forward_func(embeddings, attention_mask):\n",
    "    output = x2c_model(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1a90669-4e39-439c-9c49-f10f2557bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = ConceptGradients(forward_func, concept_forward_func=concept_forward_func, x2y_model=x2y_model, x2c_model=x2c_model)\n",
    "\n",
    "def calculate_concept_gradient(input_ids, attention_mask, target_index, concept_index, mode):\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = x2y_model.model.get_input_embeddings()(input_ids)\n",
    "    embeddings.requires_grad_(True)\n",
    "    attention_mask = attention_mask.float()\n",
    "    attention_mask.requires_grad_(True)\n",
    "    attr = cg.attribute(\n",
    "        (embeddings, attention_mask),\n",
    "        mode=mode,\n",
    "        target=target_index,\n",
    "        target_concept=concept_index,\n",
    "        n_concepts=5,\n",
    "        target_layer_name='roberta.encoder.layer.10.attention.output.dense',\n",
    "        concept_layer_name='roberta.encoder.layer.10.attention.output.dense',\n",
    "    )\n",
    "\n",
    "    return attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a977244f-22a6-4ba1-aea5-f8a7f56f9d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('dataset/test.csv')\n",
    "df_test = df_test.sample(2000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebb63e94-70bb-4990-817e-5ee8c636a7de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>concept_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>It's an idiotic system offering only one moronic choice. It lacks the integrity to offer a True/False choice, and is essentially meaningless. A lie politely expressed is bogus civility.</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 0, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>I WAS RAPED BY AMY DEMBOSKI\\n\\nLet's talk about it, either validate or disprove it</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>I guess it's hard to keep an old race-baiting hypocrite like tangreedo down...\\nHowever, if he doesn't understand the basic elements of the First Amendment, then he's not fit to hold any office. Let him waste more money running - as long as he's never voted in again.</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 0, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>Mine as well. I gather one can only speculate the race of the perpetrator if it's white.</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>I didn't see what the Steelers did.  The whole team wasn't on the field?  I thought it classy what Dallas did.  Good for them all.  Including Jerry Jones (I think I just threw up in my mouth).\\n\\nIt would be tough to dump the song entirely.  Heck, bring back the F-16 fly-by's and I'm sure we'd get over it.  ;-)\\n\\n\"...By the way, someone can be proud of being an American and also not a nationalist nazi....\"\\nYup.  Just used that wording to make being a Cowboy fan that much worse.  :-D</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 0, 1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   comment_text  \\\n",
       "6252                                                                                                                                                                                                                                                                                                                  It's an idiotic system offering only one moronic choice. It lacks the integrity to offer a True/False choice, and is essentially meaningless. A lie politely expressed is bogus civility.   \n",
       "4684                                                                                                                                                                                                                                                                                                                                                                                                                         I WAS RAPED BY AMY DEMBOSKI\\n\\nLet's talk about it, either validate or disprove it   \n",
       "1731                                                                                                                                                                                                                                I guess it's hard to keep an old race-baiting hypocrite like tangreedo down...\\nHowever, if he doesn't understand the basic elements of the First Amendment, then he's not fit to hold any office. Let him waste more money running - as long as he's never voted in again.   \n",
       "4742                                                                                                                                                                                                                                                                                                                                                                                                                   Mine as well. I gather one can only speculate the race of the perpetrator if it's white.   \n",
       "4521  I didn't see what the Steelers did.  The whole team wasn't on the field?  I thought it classy what Dallas did.  Good for them all.  Including Jerry Jones (I think I just threw up in my mouth).\\n\\nIt would be tough to dump the song entirely.  Heck, bring back the F-16 fly-by's and I'm sure we'd get over it.  ;-)\\n\\n\"...By the way, someone can be proud of being an American and also not a nationalist nazi....\"\\nYup.  Just used that wording to make being a Cowboy fan that much worse.  :-D   \n",
       "\n",
       "      toxicity   concept_labels  \n",
       "6252         1  [1, 0, 0, 1, 1]  \n",
       "4684         1  [0, 1, 1, 1, 1]  \n",
       "1731         1  [1, 0, 0, 1, 1]  \n",
       "4742         0  [0, 0, 0, 0, 0]  \n",
       "4521         1  [1, 0, 0, 1, 1]  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['concept_labels'] = df_test[['obscene', 'threat', 'sexual_explicit', 'insult', 'identity_attack']].values.tolist()\n",
    "df_test.drop(columns=['obscene', 'threat', 'insult', 'severe_toxicity', 'id', 'identity_attack', 'sexual_explicit'], inplace=True, axis=1)\n",
    "ds_test = Dataset.from_pandas(df_test)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5536b2f-73b0-42e8-aa43-0e707b27c97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9c5d29ea9844869889273d0cd45c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"comment_text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = ds_test.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"concept_labels\", \"toxicity\"])\n",
    "\n",
    "x2y_dl_test = DataLoader(tokenized_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e160794-2a83-4f78-a750-634bceaebe69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b1eeb84f474c9cace23fcab1e3dedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process complete. Results saved to 'misclassified_samples_with_concept_gradients.csv'\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for batch in tqdm(x2y_dl_test, leave=True):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['toxicity'].to(device)\n",
    "    concept_labels = batch['concept_labels']\n",
    "\n",
    "    target_logits = x2y_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    target_preds = torch.argmax(target_logits, dim=-1)\n",
    "\n",
    "    incorrect_indices = (target_preds != labels).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    if len(incorrect_indices) > 0:\n",
    "        for idx in incorrect_indices:\n",
    "            sample_input_ids = input_ids[idx].unsqueeze(0)\n",
    "            sample_attention_mask = attention_mask[idx].unsqueeze(0)\n",
    "            sample_label = labels[idx].item()\n",
    "            sample_concept_label = concept_labels[idx].unsqueeze(0)\n",
    "            sample_sentence = tokenizer.decode(sample_input_ids.squeeze(), skip_special_tokens=True)\n",
    "\n",
    "            concept_gradient = calculate_concept_gradient(sample_input_ids, sample_attention_mask, target_index=sample_label, concept_index=None, mode='chain_rule_joint')\n",
    "            concept_gradient = concept_gradient[0].detach().cpu().numpy()\n",
    "\n",
    "            concept_logits = x2c_model(input_ids=sample_input_ids, attention_mask=sample_attention_mask)\n",
    "            concept_logits = torch.sigmoid(concept_logits).detach().cpu().numpy()\n",
    "\n",
    "            target_logits_final = x2y_model(input_ids=sample_input_ids, attention_mask=sample_attention_mask)\n",
    "            target_logits_final = torch.softmax(target_logits_final, dim=-1).detach().cpu().numpy()\n",
    "\n",
    "            results.append({\n",
    "                \"sentence\": sample_sentence,\n",
    "                \"target_logits\": target_logits_final,\n",
    "                \"concept_logits\": concept_logits,\n",
    "                \"concept_gradient\": concept_gradient,\n",
    "                \"label\": sample_label,\n",
    "                \"concept_label\": sample_concept_label.cpu().numpy()  \n",
    "            })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(\"analysis_sheets/final_misclassified_samples_with_concept_gradients.csv\", index=False)\n",
    "\n",
    "print(\"Process complete. Results saved to 'misclassified_samples_with_concept_gradients.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab3f314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"classified_samples_with_concept_gradients.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db96fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_and_parse_gradient(example):\n",
    "    if isinstance(example, str):\n",
    "        cleaned_example = example.replace(' ', ',').replace(',,', ',').replace('[,', '[').replace(',]', ']')\n",
    "        try:\n",
    "            return ast.literal_eval(cleaned_example)  \n",
    "        except:\n",
    "            return None  \n",
    "    return example\n",
    "\n",
    "import numpy as np\n",
    "def mean_concepts(x):\n",
    "    return np.mean(x)\n",
    "\n",
    "df_cc = pd.read_csv('analysis_sheets/final_classified_samples_with_concept_gradients.csv')\n",
    "df_mc = pd.read_csv('analysis_sheets/final_misclassified_samples_with_concept_gradients.csv')\n",
    "df_cc['concept_gradient'] = df_cc['concept_gradient'].apply(clean_and_parse_gradient)\n",
    "df_mc['concept_gradient'] = df_mc['concept_gradient'].apply(clean_and_parse_gradient)\n",
    "df_cc.to_csv('analysis_sheets/final_classified_samples_with_concept_gradients.csv', index=False)\n",
    "df_mc.to_csv('analysis_sheets/final_misclassified_samples_with_concept_gradients.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1144f0ed-8547-419c-9b3a-d24412503464",
   "metadata": {},
   "source": [
    "### TCAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44ec4a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from scipy.stats import ttest_ind\n",
    "from captum.attr import LayerActivation\n",
    "from captum._utils.gradient import compute_layer_gradients_and_eval\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CelebA, ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchmetrics\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm, trange\n",
    "import PIL\n",
    "import seaborn\n",
    "\n",
    "def save_tcav_results(trials, tcavs, save_npz_fname=None, force=False):\n",
    "    \n",
    "    stacked_tcavs = np.stack([np.stack(list(tcavs_.values()), axis=0) for tcavs_ in tcavs], axis=0)\n",
    "    stacked_accs = np.stack([np.stack(list(trial[1].values()), axis=0)\n",
    "                             for trial in trials], axis=0)\n",
    "    \n",
    "    if save_npz_fname is not None:\n",
    "        if os.path.exists(save_npz_fname) and not force:\n",
    "            print(f\"{save_npz_fname} already exists.\")\n",
    "        else:\n",
    "            np.savez(save_npz_fname, tcavs=stacked_tcavs, accs=stacked_accs)\n",
    "    \n",
    "    return stacked_tcavs, stacked_accs\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Linear(in_dim, out_dim)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class NoReduceMSE():\n",
    "    def __init__(self):\n",
    "        self.se = 0\n",
    "        self.total = 0\n",
    "    def __call__(self, pred, gt):\n",
    "        self.se += ((pred - gt)**2).sum(0)\n",
    "        self.total += pred.shape[0]\n",
    "    def compute(self):\n",
    "        return self.se / self.total\n",
    "    \n",
    "class TCAVScore(torchmetrics.Metric):\n",
    "    def __init__(self, CAV, signed=True, dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        \n",
    "        assert len(CAV.shape) == 2\n",
    "        self.CAV = CAV\n",
    "        \n",
    "        self.signed = signed\n",
    "        \n",
    "        self.add_state(\"sum\", default=torch.zeros([self.CAV.shape[0]]), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, grads: torch.Tensor):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            assert len(grads.shape) == 2\n",
    "            assert grads.shape[-1] == self.CAV.shape[-1]\n",
    "\n",
    "            grads = grads.unsqueeze(1)\n",
    "\n",
    "            cos = F.cosine_similarity(grads, self.CAV, dim=-1)\n",
    "            if self.signed:\n",
    "                score = (cos > 0).sum(0)\n",
    "            else:\n",
    "                score = cos.sum(0)\n",
    "\n",
    "            self.sum += score\n",
    "            self.total += cos.shape[0]\n",
    "\n",
    "    def compute(self):\n",
    "        return self.sum.float() / self.total\n",
    "\n",
    "import gc\n",
    "class TCAV(nn.Module):\n",
    "    def __init__(self, target_model, layer_names=None, cache_dir=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.target_model = target_model.eval()\n",
    "        \n",
    "        self.CAVs = None\n",
    "        self.random_CAVs = None\n",
    "        self.metrics = None\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "        assert (layer_names is not None) or (cache_dir is not None)\n",
    "        \n",
    "        # reload from cache\n",
    "        if self.cache_dir is not None and \\\n",
    "            os.path.exists(os.path.join(self.cache_dir, 'random_CAVs.npz')) and \\\n",
    "            os.path.exists(os.path.join(self.cache_dir, 'CAVs.npz')) and \\\n",
    "            os.path.exists(os.path.join(self.cache_dir, 'metrics.npz')):\n",
    "\n",
    "            print(\"Loading `random_CAVs.npz`, `CAVs.npz`, and `metrics.npz` from cache...\")\n",
    "            with np.load(os.path.join(self.cache_dir, 'random_CAVs.npz')) as f:\n",
    "                random_CAVs = {k: v for k, v in f.items()}\n",
    "            assert all([len(v) > 0 for v in random_CAVs.values()])\n",
    "            self.random_CAVs = random_CAVs\n",
    "\n",
    "            with np.load(os.path.join(self.cache_dir, 'CAVs.npz')) as f:\n",
    "                CAVs = {k: v for k, v in f.items()}\n",
    "            assert all([len(v) > 0 for v in CAVs.values()])\n",
    "            self.CAVs = CAVs\n",
    "            \n",
    "            with np.load(os.path.join(self.cache_dir, 'metrics.npz')) as f:\n",
    "                metrics = {k: v for k, v in f.items()}\n",
    "            assert all([len(v) > 0 for v in metrics.values()])\n",
    "            self.metrics = metrics\n",
    "\n",
    "            assert list(self.random_CAVs.keys()) == list(self.CAVs.keys())\n",
    "            assert list(self.metrics.keys()) == list(self.CAVs.keys())\n",
    "\n",
    "            self.layer_names = list(self.random_CAVs.keys())\n",
    "            print(f\"Using cached layer names: {self.layer_names}\")\n",
    "        else:\n",
    "            self.layer_names = layer_names\n",
    "        \n",
    "        # searching for layers in target_model\n",
    "        self.layers = {}\n",
    "        # print(list(target_model.named_modules()))\n",
    "        for name, layer in target_model.named_modules():\n",
    "            if name in self.layer_names:\n",
    "                self.layers[name] = layer\n",
    "        if sorted(self.layer_names) != sorted(list(self.layers.keys())):\n",
    "            raise ValueError(f\"Keys {sorted(self.layer_names)} and {sorted(list(self.layers.keys()))} don't match.\")\n",
    "    \n",
    "    @staticmethod            \n",
    "    def get_class_balanced_sampler(ys, y_index):\n",
    "        ys = ys[:, y_index]\n",
    "        pos_ratio = ys.sum() / ys.shape[0]\n",
    "        weights = ys * (1 - pos_ratio) + (1 - ys).abs() * pos_ratio\n",
    "        sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))\n",
    "        return sampler\n",
    "\n",
    "    def _generate_CAVs(self, dset_train, dset_valid, hparams=None, verbose=True):\n",
    "        \n",
    "        default_hparams = dict(task='classification', n_epochs=100, lr=1e-4, weight_decay=1e-2, \n",
    "                               batch_size=32, patience=10, pos_weight=None, num_workers=2)\n",
    "        \n",
    "        if hparams is None:\n",
    "            hparams = default_hparams\n",
    "        else:\n",
    "            default_hparams.update(hparams)\n",
    "            hparams = default_hparams\n",
    "        \n",
    "        dl_train = DataLoader(dset_train, batch_size=hparams['batch_size'], drop_last=False, \n",
    "                              num_workers=hparams['num_workers'], shuffle=False, persistent_workers=False)\n",
    "        \n",
    "        dl_valid = DataLoader(dset_valid, batch_size=hparams['batch_size'], drop_last=False, \n",
    "                              num_workers=hparams['num_workers'], shuffle=False, persistent_workers=False)\n",
    "        \n",
    "        device = next(self.target_model.parameters())\n",
    "        cs_train = torch.cat([batch['labels'].detach().clone() for batch in tqdm(dl_train, leave=False)], dim=0)\n",
    "        cs_valid = torch.cat([batch['labels'].detach().clone() for batch in tqdm(dl_valid, leave=False)], dim=0)\n",
    "        \n",
    "        CAVs, metrics = {}, {}\n",
    "        \n",
    "        for layer_name, layer in tqdm(self.layers.items(), leave=False, desc=\"Layers: \"):\n",
    "            layer_act = LayerActivation(self.target_model, layer)\n",
    "            \n",
    "            # extract activations\n",
    "            acts_train = []\n",
    "            max_batches = 140\n",
    "            for batch_idx, batch in enumerate(tqdm(dl_train, leave=False)):\n",
    "                try:\n",
    "                    # if batch_idx >= max_batches:\n",
    "                    #     break\n",
    "                    input_ids = batch['input_ids'].to(device).long()\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['labels'].to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        embeddings = self.target_model.model.get_input_embeddings()(input_ids)\n",
    "                    \n",
    "                    embeddings.requires_grad_(True)\n",
    "                    attention_mask = attention_mask.float()\n",
    "                    attention_mask.requires_grad_(True)\n",
    "                    act = layer_act.attribute((embeddings, attention_mask), attribute_to_layer_input=True).flatten(start_dim=1)\n",
    "\n",
    "                    np.save(os.path.join('cav', f\"cav_acts/act_batch_{batch_idx}.npy\"), act.detach().cpu().numpy())\n",
    "                    # acts_train.append(act.detach().cpu().numpy())\n",
    "                    del act, embeddings, attention_mask, input_ids, labels\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping batch due to error: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "            acts_train = [np.load(os.path.join('cav', f\"cav_acts/act_batch_{i}.npy\")) for i in range(len(dl_train))]\n",
    "            acts_train = [torch.tensor(act, dtype=torch.float32) for act in acts_train]\n",
    "\n",
    "            # Now concatenate the list of tensors\n",
    "            acts_train = torch.cat(acts_train, dim=0)\n",
    "            layer_dset_train = torch.utils.data.TensorDataset(acts_train, cs_train)\n",
    "            acts_valid = []\n",
    "            for batch_idx, batch in enumerate(tqdm(dl_valid, leave=False)):\n",
    "                try:\n",
    "                    input_ids = batch['input_ids'].to(device).long()\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['labels'].to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        embeddings = self.target_model.model.get_input_embeddings()(input_ids)\n",
    "                        \n",
    "                    embeddings.requires_grad_(True)\n",
    "                    attention_mask = attention_mask.float()\n",
    "                    attention_mask.requires_grad_(True)\n",
    "                    act = layer_act.attribute((embeddings, attention_mask), attribute_to_layer_input=True).flatten(start_dim=1)\n",
    "                    in_dim, out_dim = act.shape[1], labels.shape[1]\n",
    "                    np.save(os.path.join('cav', f\"cav_acts_val/act_batch_{batch_idx}.npy\"), act.detach().cpu().numpy())\n",
    "                    del act, embeddings, attention_mask, input_ids, labels  \n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping batch due to error: {e}\")\n",
    "                    continue\n",
    "                \n",
    "            acts_valid = [np.load(os.path.join('cav', f'cav_acts_val/act_batch_{i}.npy')) for i in range(len(dl_valid))]\n",
    "            acts_valid = [torch.tensor(act, dtype=torch.float32) for act in acts_valid]\n",
    "\n",
    "            # Now concatenate the list of tensors\n",
    "            acts_valid = torch.cat(acts_valid, dim=0)\n",
    "            # acts_valid = torch.cat(acts_valid, dim=0)\n",
    "            \n",
    "            layer_dset_valid = torch.utils.data.TensorDataset(acts_valid, cs_valid)\n",
    "            \n",
    "            if hparams['pos_weight'] is not None:\n",
    "                if isinstance(hparams['pos_weight'], torch.Tensor):\n",
    "                    pos_weight = hparams['pos_weight'].to(device)\n",
    "                else:\n",
    "                    pos_weight = hparams['pos_weight'] * torch.ones([out_dim]).to(device)\n",
    "            else:\n",
    "                pos_weight = None\n",
    "\n",
    "            # sampler = get_class_balanced_sampler(all_cs, nc)\n",
    "            layer_dl_train = DataLoader(layer_dset_train, batch_size=hparams['batch_size'], drop_last=False, \n",
    "                                        num_workers=hparams['num_workers'], shuffle=True)\n",
    "            layer_dl_valid = DataLoader(layer_dset_valid, batch_size=hparams['batch_size'], drop_last=False, \n",
    "                                        num_workers=hparams['num_workers'], shuffle=False)\n",
    "\n",
    "            # define model and optimizer\n",
    "            linear_model = LinearModel(in_dim, out_dim).to(device)\n",
    "            optimizer = optim.Adam(linear_model.parameters(), lr=hparams['lr'], \n",
    "                                   weight_decay=hparams['weight_decay'])\n",
    "            if hparams['task'] == 'classification':\n",
    "                loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "                metric = torchmetrics.Accuracy(threshold=0.0, task = 'multilabel', num_classes=out_dim, num_labels=6, average=None).to(device)\n",
    "            elif hparams['task'] == 'regression':\n",
    "                loss_fn = torch.nn.MSELoss()\n",
    "                metric = NoReduceMSE()\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            # train\n",
    "            patience = 0\n",
    "            min_loss = np.inf\n",
    "            linear_model.train()\n",
    "            with trange(hparams['n_epochs'], leave=False, desc=\"Epochs: \") as tepochs:\n",
    "                for epoch in tepochs:\n",
    "                    losses = []\n",
    "                    for xs, cs in layer_dl_train:\n",
    "                        xs = xs.to(device)\n",
    "                        cs = cs.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        logits_cs = linear_model(xs)\n",
    "                        loss = loss_fn(logits_cs, cs.float())\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        losses.append(loss.item())\n",
    "                    \n",
    "                    if min_loss > np.mean(losses):\n",
    "                        min_loss = np.mean(losses)\n",
    "                        patience = 0\n",
    "                    else:\n",
    "                        patience += 1\n",
    "                    tepochs.set_postfix(loss=f\"{np.mean(losses):.4f}/{min_loss:.4f}\")\n",
    "                    sleep(0.1)\n",
    "                    if patience > hparams['patience']:\n",
    "                        tepochs.update(n=hparams['n_epochs'] - epoch)\n",
    "                        tepochs.close()\n",
    "                        break\n",
    "\n",
    "            # eval\n",
    "            linear_model.eval()\n",
    "            for xs, cs in layer_dl_valid:\n",
    "                xs = xs.to(device)\n",
    "                cs = cs.to(device).to(cs_valid.dtype)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    pred_cs = linear_model(xs)\n",
    "                    metric(pred_cs, cs)\n",
    "            \n",
    "            CAV = linear_model.model.weight.detach().clone()\n",
    "            CAV = CAV / torch.norm(CAV, dim=1, keepdim=True)\n",
    "            CAVs[layer_name] = CAV.cpu().numpy()\n",
    "            metrics[layer_name] = metric.compute().detach().cpu().numpy()\n",
    "            \n",
    "        return CAVs, metrics\n",
    "    \n",
    "    def _generate_random_CAVs(self, dset_train, dset_valid):\n",
    "        dl = DataLoader(dset_train, batch_size=32, drop_last=False, \n",
    "                        num_workers=8, shuffle=False)\n",
    "        device = next(self.target_model.parameters())\n",
    "        \n",
    "        CAVs = {}\n",
    "        for layer_name, layer in tqdm(self.layers.items(), leave=False):\n",
    "            layer_act = LayerActivation(self.target_model, layer)\n",
    "            # get in_dim\n",
    "            for batch in dl:\n",
    "                input_ids = batch['input_ids'].to(device).long()\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    embeddings = self.target_model.model.get_input_embeddings()(input_ids)\n",
    "                    \n",
    "                embeddings.requires_grad_(True)\n",
    "                attention_mask = attention_mask.float()\n",
    "                attention_mask.requires_grad_(True)\n",
    "                act = layer_act.attribute((embeddings, attention_mask), attribute_to_layer_input=True).flatten(start_dim=1)\n",
    "                in_dim, out_dim = act.shape[1], labels.shape[1]\n",
    "                break\n",
    "            CAV = (torch.rand(out_dim, in_dim) - 1)\n",
    "            CAV = CAV / torch.norm(CAV, dim=1, keepdim=True)\n",
    "            CAVs[layer_name] = CAV.cpu().numpy()\n",
    "        \n",
    "        return CAVs\n",
    "    \n",
    "    def generate_CAVs(self, dset_train, dset_valid, n_repeat=5, hparams=None, force_rewrite_cache=False):\n",
    "        \n",
    "        self.CAVs = {layer_name: [] for layer_name in self.layer_names}\n",
    "        metrics = {layer_name: [] for layer_name in self.layer_names}\n",
    "        \n",
    "        # reload from cache\n",
    "        if (self.cache_dir is not None) and (not force_rewrite_cache) and \\\n",
    "           (os.path.exists(os.path.join(self.cache_dir, 'CAVs.npz'))) and \\\n",
    "           (os.path.exists(os.path.join(self.cache_dir, 'metrics.npz'))):\n",
    "            \n",
    "            raise ValueError(\"Cached directory already exist. Use `force_rewrite_cache = True` to overwrite.\")\n",
    "            '''\n",
    "            print(\"Loading from cache...\")\n",
    "            \n",
    "            with np.load(os.path.join(self.cache_dir, 'CAVs.npz')) as f:\n",
    "                self.CAVs.update({k: v for k, v in f.items()})\n",
    "            with np.load(os.path.join(self.cache_dir, 'metrics.npz')) as f:\n",
    "                metrics.update({k: v for k, v in f.items()})\n",
    "            \n",
    "            if all([len(v) > 0 for v in self.CAVs.values()]):\n",
    "                return self.CAVs, metrics\n",
    "            '''\n",
    "        \n",
    "        update_layer_names = [k for k, v in self.CAVs.items() if len(v) == 0]\n",
    "        print(f\"Generating TCAV for layers: {update_layer_names}\")\n",
    "        \n",
    "        # generate\n",
    "        for _ in trange(n_repeat, desc=\"#repeats: \"):\n",
    "            CAVs_, metrics_ = self._generate_CAVs(dset_train, dset_valid, hparams=hparams)\n",
    "            for layer_name in update_layer_names:\n",
    "                self.CAVs[layer_name].append(CAVs_[layer_name])\n",
    "                metrics[layer_name].append(metrics_[layer_name])\n",
    "                \n",
    "        for layer_name in update_layer_names:\n",
    "            self.CAVs[layer_name] = np.stack(self.CAVs[layer_name], axis=0)\n",
    "            metrics[layer_name] = np.stack(metrics[layer_name], axis=0)\n",
    "\n",
    "        if self.cache_dir is not None:\n",
    "            os.makedirs(self.cache_dir, exist_ok=True)\n",
    "            np.savez_compressed(os.path.join(self.cache_dir, 'CAVs.npz'), **self.CAVs)\n",
    "            np.savez_compressed(os.path.join(self.cache_dir, 'metrics.npz'), **metrics)\n",
    "            \n",
    "        return self.CAVs, metrics\n",
    "    \n",
    "    def generate_random_CAVs(self, dset_train, dset_valid, n_repeat=5, force_rewrite_cache=False):\n",
    "        \n",
    "        random_CAVs = {layer_name: [] for layer_name in self.layer_names}\n",
    "        \n",
    "        # reload from cache\n",
    "        if (self.cache_dir is not None) and (not force_rewrite_cache) and \\\n",
    "           (os.path.exists(os.path.join(self.cache_dir, 'random_CAVs.npz'))):\n",
    "            \n",
    "            raise ValueError(\"Cached directory already exist. Use `force_rewrite_cache = True` to overwrite.\")\n",
    "            \n",
    "            '''\n",
    "            print(\"Loading from cache...\")\n",
    "            \n",
    "            with np.load(os.path.join(self.cache_dir, 'random_CAVs.npz')) as f:\n",
    "                random_CAVs.update({k: v for k, v in f.items()})\n",
    "            \n",
    "            if all([len(v) > 0 for v in random_CAVs.values()]):\n",
    "                return random_CAVs\n",
    "            '''\n",
    "        \n",
    "        update_layer_names = [k for k, v in random_CAVs.items() if len(v) == 0]\n",
    "        print(f\"Generating random TCAV for layers: {update_layer_names}\")\n",
    "        \n",
    "        for _ in trange(n_repeat):\n",
    "            random_CAVs_ = self._generate_random_CAVs(dset_train, dset_valid)\n",
    "            for layer_name in update_layer_names:\n",
    "                random_CAVs[layer_name].append(random_CAVs_[layer_name])\n",
    "                \n",
    "        for layer_name in update_layer_names:\n",
    "            random_CAVs[layer_name] = np.stack(random_CAVs[layer_name], axis=0)\n",
    "\n",
    "        if self.cache_dir is not None:\n",
    "            os.makedirs(self.cache_dir, exist_ok=True) \n",
    "            np.savez_compressed(os.path.join(self.cache_dir, 'random_CAVs.npz'), **random_CAVs)\n",
    "        \n",
    "        self.random_CAVs = random_CAVs\n",
    "        return self.random_CAVs\n",
    "    \n",
    "    def generate_TCAVs(self, dset_valid, layer_name, target_index=None, score_signed=True, \n",
    "                       return_ttest_results=False, ttest_threshold=0.05):\n",
    "        \n",
    "        assert len(self.CAVs[layer_name]) == len(self.random_CAVs[layer_name])\n",
    "        n_repeat = len(self.CAVs[layer_name])\n",
    "        \n",
    "        device = next(self.target_model.parameters())\n",
    "        tcavs, random_tcavs = [], []\n",
    "        \n",
    "        concept_dl = DataLoader(dset_valid, batch_size=16, shuffle=False, \n",
    "                                drop_last=False, num_workers=8)\n",
    "        \n",
    "        for i in trange(n_repeat, leave=False):\n",
    "            \n",
    "            CAV = torch.from_numpy(self.CAVs[layer_name][i]).float().to(device)\n",
    "            random_CAV = torch.from_numpy(self.random_CAVs[layer_name][i]).float().to(device)\n",
    "\n",
    "            tcavs_ = TCAVScore(CAV, signed=score_signed).to(device)\n",
    "            random_tcavs_ = TCAVScore(random_CAV, signed=score_signed).to(device)\n",
    "\n",
    "            for xs, cs in concept_dl:\n",
    "                xs = xs.to(device)\n",
    "                cs = cs.to(device)\n",
    "\n",
    "                layer_grads_, _ = compute_layer_gradients_and_eval(\n",
    "                    self.target_model, layer, xs, target_ind=target_index, \n",
    "                    attribute_to_layer_input=True)\n",
    "                del _\n",
    "                layer_grads_ = layer_grads_[0].flatten(start_dim=1)\n",
    "\n",
    "                tcavs_(layer_grads_)\n",
    "                random_tcavs_(layer_grads_)\n",
    "\n",
    "            tcavs.append(tcavs_.compute().detach().cpu().numpy())\n",
    "            random_tcavs.append(random_tcavs_.compute().detach().cpu().numpy())\n",
    "        \n",
    "        random_tcavs = np.stack(random_tcavs, axis=0)\n",
    "        tcavs = np.stack(tcavs, axis=0)\n",
    "        \n",
    "        print('random_tcavs:', random_tcavs.mean(0))\n",
    "        print('tcavs:', tcavs.mean(0))\n",
    "        \n",
    "        # run two-sided test\n",
    "        ttest_results = []\n",
    "        for i in range(tcavs.shape[1]):\n",
    "            ttest_result = ttest_ind(tcavs[:, i], random_tcavs[:, i])\n",
    "            ttest_results.append(ttest_result.pvalue)\n",
    "        ttest_results = np.array(ttest_results)\n",
    "        \n",
    "        avg_tcav_scores = tcavs.mean(0)\n",
    "        avg_tcav_scores[~(ttest_results < ttest_threshold)] = np.nan\n",
    "        \n",
    "        if return_ttest_results:\n",
    "            return avg_tcav_scores, ttest_results\n",
    "        else:\n",
    "            return avg_tcav_scores\n",
    "\n",
    "    def attribute(self, inputs, layer_name, mode, target=None, abs=False, use_random=False, select_index=None):\n",
    "        \n",
    "        assert mode in ['inner_prod', 'cosine_similarity']\n",
    "        \n",
    "        \n",
    "        if use_random:\n",
    "            assert self.random_CAVs is not None\n",
    "            \n",
    "            if select_index is None:\n",
    "                CAV_ = self.random_CAVs[layer_name].mean(0)\n",
    "            else:\n",
    "                CAV_ = self.random_CAVs[layer_name][select_index]\n",
    "        else:\n",
    "            assert self.CAVs is not None\n",
    "            \n",
    "            if select_index is None:\n",
    "                CAV_ = self.CAVs[layer_name].mean(0)\n",
    "            else:\n",
    "                CAV_ = self.CAVs[layer_name][select_index]\n",
    "        CAV = torch.from_numpy(CAV_).float().to(device)\n",
    "        # print(CAV)\n",
    "        # CAV.float().to(inputs.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            grads, _ = compute_layer_gradients_and_eval(\n",
    "                self.target_model, self.layers[layer_name], inputs, \n",
    "                target_ind=target, attribute_to_layer_input=True)\n",
    "            del _\n",
    "            grads = grads[0].flatten(start_dim=1)\n",
    "            \n",
    "            if mode == 'inner_prod':\n",
    "                attributions = grads @ CAV.T\n",
    "            elif mode == 'cosine_similarity':\n",
    "                attributions = grads @ CAV.T / (torch.norm(grads, dim=1, keepdim=True) * \\\n",
    "                                                torch.norm(CAV.T, dim=0, keepdim=True))\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "            if abs:\n",
    "                attributions = torch.abs(attributions)\n",
    "                \n",
    "        return attributions\n",
    "    \n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class X2YModel(nn.Module):\n",
    "    def __init__(self, model_name='saved_target_model', num_classes=2):\n",
    "        super(X2YModel, self).__init__()\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, inputs_embeds=None):\n",
    "        if inputs_embeds is not None:\n",
    "            outputs = self.model.roberta(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        else:\n",
    "\n",
    "            outputs = self.model.roberta(inputs_embeds=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        return self.model.classifier(outputs.last_hidden_state)  \n",
    "\n",
    "x2y_model = X2YModel().to(device)\n",
    "\n",
    "chosen_model = x2y_model#x2c_model\n",
    "chosen_cache_dir = 'cav'#'cav_x2c'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "449c3106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e727c2aaddc04b40a9c34a700a68820e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1886 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num concepts: 5\n",
      "Loading `random_CAVs.npz`, `CAVs.npz`, and `metrics.npz` from cache...\n",
      "Using cached layer names: ['model.roberta.encoder.layer.10.attention.output.dense']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d6fbb9567144ebae0f172abf51e2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.00142059, -0.00031433, 0.00088187, 0.0011738, -0.00229424]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Its for spreadsheet\n",
    "\n",
    "layer_names = ['model.roberta.encoder.layer.10.attention.output.dense']#, 'model.roberta.encoder.layer.10.attention.output.dense', 'model.roberta.encoder.layer.9.attention.output.dense', 'model.roberta.encoder.layer.11.output.dense']\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from datasets import Dataset\n",
    "\n",
    "df_test = df_cc#pd.read_csv(\"analyze_score_cc.csv\")\n",
    "ds_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = ds_test.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x2y_dl_test = DataLoader(tokenized_dataset, batch_size=8, shuffle=False)\n",
    "n_concepts = 5\n",
    "print(f'Num concepts: {n_concepts}')\n",
    "tcav = TCAV(x2y_model, layer_names=['model.roberta.encoder.layer.10.attention.output.dense'], cache_dir='cav')\n",
    "\n",
    "attrs = []\n",
    "\n",
    "for batch in tqdm(x2y_dl_test, leave=True):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = x2y_model.model.get_input_embeddings()(input_ids)\n",
    "        # embeddings = x2c_model.model.get_input_embeddings()(input_ids)\n",
    "        \n",
    "    embeddings.requires_grad_(True)\n",
    "    attention_mask = attention_mask.float()\n",
    "    attention_mask.requires_grad_(True)\n",
    "    y = batch['label'].to(device)\n",
    "    attr = tcav.attribute((embeddings, attention_mask), 'model.roberta.encoder.layer.10.attention.output.dense', 'cosine_similarity', target=y)\n",
    "    attr = attr.detach().cpu().numpy()\n",
    "    attrs.append(attr)\n",
    "\n",
    "attrs = np.concatenate(attrs, axis=0)\n",
    "\n",
    "np.save('cav/attr_npy_classification.npy', attrs)\n",
    "\n",
    "\n",
    "attrs = np.load('cav/attr_npy_classification.npy')\n",
    "# df.shape, attrs.shape\n",
    "df = df_test.copy()\n",
    "# another column for the tcav-scores using attrs\n",
    "# df['tcav_score'] =  np.nan\n",
    "# df['tcav_score'] = attrs\n",
    "# df.tcav_score should contain a list of all the tcav scores for each concept\n",
    "df['tcav_score'] = attrs.tolist()\n",
    "# reduce the precision of the tcav scores\n",
    "df.tcav_score = df.tcav_score.apply(lambda x: [round(i, 8) for i in x])\n",
    "df.tcav_score[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "473324f3-b1bc-481b-b970-deba046dd916",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('analysis_sheets/final_classified_samples_with_concept_gradients.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91664f0d-b7ee-4d66-8752-a9964f32ca7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>target_logits</th>\n",
       "      <th>concept_logits</th>\n",
       "      <th>concept_gradient</th>\n",
       "      <th>label</th>\n",
       "      <th>concept_label</th>\n",
       "      <th>tcav_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's an idiotic system offering only one moronic choice. It lacks the integrity to offer a True/False choice, and is essentially meaningless. A lie politely expressed is bogus civility.</td>\n",
       "      <td>[[0.05489601 0.9617731 ]]</td>\n",
       "      <td>[[0.99932384 0.23486891 0.756429   0.9986303  0.18379843]]</td>\n",
       "      <td>[[0.04189841, -0.02674945, 0.02142245, -0.00010259, 0.06312781]]</td>\n",
       "      <td>1</td>\n",
       "      <td>[[1 0 0 1 1]]</td>\n",
       "      <td>[0.00142059, -0.00031433, 0.00088187, 0.0011738, -0.00229424]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I WAS RAPED BY AMY DEMBOSKI\\n\\nLet's talk about it, either validate or disprove it</td>\n",
       "      <td>[[0.2097058 0.8583711]]</td>\n",
       "      <td>[[0.99266475 0.99864525 0.8775576  0.9963966  0.99759716]]</td>\n",
       "      <td>[[0.07840082, -0.13348918, 0.11335696, -0.05296306, -0.08456826]]</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0 1 1 1 1]]</td>\n",
       "      <td>[0.00129855, 0.00147589, -0.00079129, 0.00112493, -0.00903469]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I guess it's hard to keep an old race-baiting hypocrite like tangreedo down...\\nHowever, if he doesn't understand the basic elements of the First Amendment, then he's not fit to hold any office. Let him waste more money running - as long as he's never voted in again.</td>\n",
       "      <td>[[0.13946097 0.9065232 ]]</td>\n",
       "      <td>[[0.8774967  0.04580401 0.92773587 0.9996799  0.09558382]]</td>\n",
       "      <td>[[0.06866927, -0.09557968, 0.11821295, 0.14562763, -0.06304058]]</td>\n",
       "      <td>1</td>\n",
       "      <td>[[1 0 0 1 1]]</td>\n",
       "      <td>[-0.00100404, -0.0036003, -0.00078455, -0.00366171, -0.00616127]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I find it hard to take this congresswoman clown serious, she looks like a transgendered cowboy/girl. What a joker!</td>\n",
       "      <td>[[0.05246165 0.9623934 ]]</td>\n",
       "      <td>[[0.9769464  0.59346807 0.9991326  0.99851865 0.2192553 ]]</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>[[1 0 1 1 1]]</td>\n",
       "      <td>[0.00286534, -0.00363649, 0.00037329, 0.00138659, -0.00115875]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sounds like a loser.  Or at least, an angry loser, lol</td>\n",
       "      <td>[[0.04923106 0.96299654]]</td>\n",
       "      <td>[[0.9166289  0.04369685 0.18025956 0.999726   0.12345581]]</td>\n",
       "      <td>[[0.02209041, -0.00520486, 0.00913896, -0.01408261, 0.02067645]]</td>\n",
       "      <td>1</td>\n",
       "      <td>[[1 1 0 1 0]]</td>\n",
       "      <td>[-0.00015554, -0.0024519, -0.00013447, -4.24e-06, -0.00194035]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                      sentence  \\\n",
       "0                                                                                    It's an idiotic system offering only one moronic choice. It lacks the integrity to offer a True/False choice, and is essentially meaningless. A lie politely expressed is bogus civility.   \n",
       "1                                                                                                                                                                                           I WAS RAPED BY AMY DEMBOSKI\\n\\nLet's talk about it, either validate or disprove it   \n",
       "2  I guess it's hard to keep an old race-baiting hypocrite like tangreedo down...\\nHowever, if he doesn't understand the basic elements of the First Amendment, then he's not fit to hold any office. Let him waste more money running - as long as he's never voted in again.   \n",
       "3                                                                                                                                                           I find it hard to take this congresswoman clown serious, she looks like a transgendered cowboy/girl. What a joker!   \n",
       "4                                                                                                                                                                                                                       Sounds like a loser.  Or at least, an angry loser, lol   \n",
       "\n",
       "               target_logits  \\\n",
       "0  [[0.05489601 0.9617731 ]]   \n",
       "1    [[0.2097058 0.8583711]]   \n",
       "2  [[0.13946097 0.9065232 ]]   \n",
       "3  [[0.05246165 0.9623934 ]]   \n",
       "4  [[0.04923106 0.96299654]]   \n",
       "\n",
       "                                               concept_logits  \\\n",
       "0  [[0.99932384 0.23486891 0.756429   0.9986303  0.18379843]]   \n",
       "1  [[0.99266475 0.99864525 0.8775576  0.9963966  0.99759716]]   \n",
       "2  [[0.8774967  0.04580401 0.92773587 0.9996799  0.09558382]]   \n",
       "3  [[0.9769464  0.59346807 0.9991326  0.99851865 0.2192553 ]]   \n",
       "4  [[0.9166289  0.04369685 0.18025956 0.999726   0.12345581]]   \n",
       "\n",
       "                                                    concept_gradient  label  \\\n",
       "0   [[0.04189841, -0.02674945, 0.02142245, -0.00010259, 0.06312781]]      1   \n",
       "1  [[0.07840082, -0.13348918, 0.11335696, -0.05296306, -0.08456826]]      1   \n",
       "2   [[0.06866927, -0.09557968, 0.11821295, 0.14562763, -0.06304058]]      1   \n",
       "3                                                               None      1   \n",
       "4   [[0.02209041, -0.00520486, 0.00913896, -0.01408261, 0.02067645]]      1   \n",
       "\n",
       "   concept_label  \\\n",
       "0  [[1 0 0 1 1]]   \n",
       "1  [[0 1 1 1 1]]   \n",
       "2  [[1 0 0 1 1]]   \n",
       "3  [[1 0 1 1 1]]   \n",
       "4  [[1 1 0 1 0]]   \n",
       "\n",
       "                                                         tcav_score  \n",
       "0     [0.00142059, -0.00031433, 0.00088187, 0.0011738, -0.00229424]  \n",
       "1    [0.00129855, 0.00147589, -0.00079129, 0.00112493, -0.00903469]  \n",
       "2  [-0.00100404, -0.0036003, -0.00078455, -0.00366171, -0.00616127]  \n",
       "3    [0.00286534, -0.00363649, 0.00037329, 0.00138659, -0.00115875]  \n",
       "4    [-0.00015554, -0.0024519, -0.00013447, -4.24e-06, -0.00194035]  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02eabffb-f267-4455-a181-0bd9c1743a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>target_logits</th>\n",
       "      <th>concept_logits</th>\n",
       "      <th>concept_gradient</th>\n",
       "      <th>label</th>\n",
       "      <th>concept_label</th>\n",
       "      <th>tcav_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>What is worse, ticks or liberals?  They both are blood suckers!</td>\n",
       "      <td>[[0.05387298 0.9611835 ]]</td>\n",
       "      <td>[[0.99850523 0.88956136 0.99865794 0.9990465  0.55715215]]</td>\n",
       "      <td>[[0.11190026, -0.00711629, 0.10685235, -0.01391387, -0.0223923]]</td>\n",
       "      <td>1</td>\n",
       "      <td>[[1 0 0 1 1]]</td>\n",
       "      <td>[0.00255218, -0.00270456, 0.00349239, 0.00188917, -0.00032008]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>Forwards, more likely..............................</td>\n",
       "      <td>[[0.92232734 0.09227892]]</td>\n",
       "      <td>[[0.00063929 0.00078733 0.00080577 0.00072127 0.0007089 ]]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0 0 0 0 0]]</td>\n",
       "      <td>[0.00198666, 0.00461079, 0.00252969, 0.00196735, 0.00063642]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>Remember also what Jesus says about giving scandal, and the millstone.</td>\n",
       "      <td>[[0.92169785 0.10087423]]</td>\n",
       "      <td>[[0.00056885 0.00086256 0.00084678 0.00065399 0.00081226]]</td>\n",
       "      <td>[[-0.14696527, 0.02438674, -0.0330714, -0.14015268, -0.05634261]]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0 0 0 0 0]]</td>\n",
       "      <td>[0.00304911, -0.00279948, -0.00223824, -0.00041029, -0.00215613]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>I guess the Ontario liberals and Alberta ndp share stupid ideas.</td>\n",
       "      <td>[[0.05040782 0.9641423 ]]</td>\n",
       "      <td>[[0.99807763 0.09881116 0.97711986 0.99898475 0.30146742]]</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>[[1 0 0 1 1]]</td>\n",
       "      <td>[-0.00174784, 0.00041132, 0.00098502, -0.00068852, -0.00106911]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>Or buy a one way ticket...</td>\n",
       "      <td>[[0.9293104  0.07693341]]</td>\n",
       "      <td>[[0.00051418 0.00064927 0.00082675 0.00082647 0.00081356]]</td>\n",
       "      <td>[[0.17667936, 0.2650457, 0.12466749, 0.14965369, 0.15520136]]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0 0 0 0 0]]</td>\n",
       "      <td>[0.00550162, 0.00096809, 0.00549318, 0.004867, 0.00254748]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>You can say that since you have already left Canada, funny thing is you are making a living on Chinese soil.</td>\n",
       "      <td>[[0.8987819  0.11612054]]</td>\n",
       "      <td>[[0.00037696 0.00054174 0.0018553  0.00131972 0.00098301]]</td>\n",
       "      <td>[[-0.5158155, -0.2566018, -0.06607198, -0.22067684, -0.2235776]]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0 0 0 0 0]]</td>\n",
       "      <td>[0.00450565, -0.00639469, -0.00578705, -0.00338063, 0.00805975]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>Eddy deserves the hangman's noose!  Hang him as high as his clearance and a little more.  A traitor is a traitor.</td>\n",
       "      <td>[[0.08954755 0.94145256]]</td>\n",
       "      <td>[[0.94592726 0.1085472  0.7380514  0.9967552  0.99913967]]</td>\n",
       "      <td>[[-0.01484637, 0.29299054, 0.13260087, -0.24077478, -0.41638708]]</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0 1 0 1 0]]</td>\n",
       "      <td>[-0.00172819, -0.00179959, 0.00091168, -0.00319312, -0.00376357]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>You have history upside down and inside out. Are you talking about the Missiles to Megawatts program where the US bought uranium FROM Russia to power OUR country? \\n\\nhttps://en.wikipedia.org/wiki/Megatons_to_Megawatts_Program\\n\\nThat program was widely praised as a swords-to-plowshares type of initiative that was beneficial and helped reduce the aging nuclear stockpile. \\n\\nIt's as if your facts got put in a blender and you typed what poured out.</td>\n",
       "      <td>[[0.93097913 0.07181426]]</td>\n",
       "      <td>[[0.00061594 0.0007757  0.00071952 0.00066045 0.00070036]]</td>\n",
       "      <td>[[-0.46955225, -0.37156785, -0.35248777, -0.62343365, -0.45276725]]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0 0 0 0 0]]</td>\n",
       "      <td>[0.00162354, 0.00743648, 0.00128449, 0.00285041, 0.00210718]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>OK, you're right, \"x\" the texting.  I never text when I drive, regardless of circumstances.  But I might hit the speed dial to make a phone call if I'm not in traffic.\\n\\nMy pet peeve is, \"Why to I have to wait for the light to change when there's no one coming from any direction?\"  Common sense is against the law.\\n\\nThen there's that speed trap in Thorton on Washington between 88th and 84th where the speed limit drops from 45 to 35 for 4-5 blocks with no change in driving condtions.  I saw six cars pulled over in that stretch at the same time.</td>\n",
       "      <td>[[0.9369667  0.06480987]]</td>\n",
       "      <td>[[0.00062289 0.00083278 0.00070992 0.00062419 0.00071651]]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0 0 0 0 0]]</td>\n",
       "      <td>[0.00402275, 0.00629554, 0.0015151, 0.00314647, -0.00208538]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>Also, the PM is equating a lone wolf attack with ISIS, etc who commit terrorism, they say, because it's part of their long-term strategy to create a global caliphate.  The PM conveniently ignores that difference.</td>\n",
       "      <td>[[0.74431205 0.2967079 ]]</td>\n",
       "      <td>[[0.00049197 0.00061045 0.00092101 0.00116962 0.00068518]]</td>\n",
       "      <td>[[-0.3030023, -0.14486581, -0.21068025, -0.15728547, -0.2243253]]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0 0 0 0 0]]</td>\n",
       "      <td>[0.00380062, 0.00455174, -0.00263239, 0.00392534, 0.00875435]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     sentence  \\\n",
       "1177                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          What is worse, ticks or liberals?  They both are blood suckers!   \n",
       "1238                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Forwards, more likely..............................   \n",
       "774                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Remember also what Jesus says about giving scandal, and the millstone.   \n",
       "85                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           I guess the Ontario liberals and Alberta ndp share stupid ideas.   \n",
       "993                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Or buy a one way ticket...   \n",
       "1746                                                                                                                                                                                                                                                                                                                                                                                                                                                             You can say that since you have already left Canada, funny thing is you are making a living on Chinese soil.   \n",
       "577                                                                                                                                                                                                                                                                                                                                                                                                                                                         Eddy deserves the hangman's noose!  Hang him as high as his clearance and a little more.  A traitor is a traitor.   \n",
       "192                                                                                                       You have history upside down and inside out. Are you talking about the Missiles to Megawatts program where the US bought uranium FROM Russia to power OUR country? \\n\\nhttps://en.wikipedia.org/wiki/Megatons_to_Megawatts_Program\\n\\nThat program was widely praised as a swords-to-plowshares type of initiative that was beneficial and helped reduce the aging nuclear stockpile. \\n\\nIt's as if your facts got put in a blender and you typed what poured out.   \n",
       "88    OK, you're right, \"x\" the texting.  I never text when I drive, regardless of circumstances.  But I might hit the speed dial to make a phone call if I'm not in traffic.\\n\\nMy pet peeve is, \"Why to I have to wait for the light to change when there's no one coming from any direction?\"  Common sense is against the law.\\n\\nThen there's that speed trap in Thorton on Washington between 88th and 84th where the speed limit drops from 45 to 35 for 4-5 blocks with no change in driving condtions.  I saw six cars pulled over in that stretch at the same time.   \n",
       "430                                                                                                                                                                                                                                                                                                                                                      Also, the PM is equating a lone wolf attack with ISIS, etc who commit terrorism, they say, because it's part of their long-term strategy to create a global caliphate.  The PM conveniently ignores that difference.   \n",
       "\n",
       "                  target_logits  \\\n",
       "1177  [[0.05387298 0.9611835 ]]   \n",
       "1238  [[0.92232734 0.09227892]]   \n",
       "774   [[0.92169785 0.10087423]]   \n",
       "85    [[0.05040782 0.9641423 ]]   \n",
       "993   [[0.9293104  0.07693341]]   \n",
       "1746  [[0.8987819  0.11612054]]   \n",
       "577   [[0.08954755 0.94145256]]   \n",
       "192   [[0.93097913 0.07181426]]   \n",
       "88    [[0.9369667  0.06480987]]   \n",
       "430   [[0.74431205 0.2967079 ]]   \n",
       "\n",
       "                                                  concept_logits  \\\n",
       "1177  [[0.99850523 0.88956136 0.99865794 0.9990465  0.55715215]]   \n",
       "1238  [[0.00063929 0.00078733 0.00080577 0.00072127 0.0007089 ]]   \n",
       "774   [[0.00056885 0.00086256 0.00084678 0.00065399 0.00081226]]   \n",
       "85    [[0.99807763 0.09881116 0.97711986 0.99898475 0.30146742]]   \n",
       "993   [[0.00051418 0.00064927 0.00082675 0.00082647 0.00081356]]   \n",
       "1746  [[0.00037696 0.00054174 0.0018553  0.00131972 0.00098301]]   \n",
       "577   [[0.94592726 0.1085472  0.7380514  0.9967552  0.99913967]]   \n",
       "192   [[0.00061594 0.0007757  0.00071952 0.00066045 0.00070036]]   \n",
       "88    [[0.00062289 0.00083278 0.00070992 0.00062419 0.00071651]]   \n",
       "430   [[0.00049197 0.00061045 0.00092101 0.00116962 0.00068518]]   \n",
       "\n",
       "                                                         concept_gradient  \\\n",
       "1177     [[0.11190026, -0.00711629, 0.10685235, -0.01391387, -0.0223923]]   \n",
       "1238                                                                 None   \n",
       "774     [[-0.14696527, 0.02438674, -0.0330714, -0.14015268, -0.05634261]]   \n",
       "85                                                                   None   \n",
       "993         [[0.17667936, 0.2650457, 0.12466749, 0.14965369, 0.15520136]]   \n",
       "1746     [[-0.5158155, -0.2566018, -0.06607198, -0.22067684, -0.2235776]]   \n",
       "577     [[-0.01484637, 0.29299054, 0.13260087, -0.24077478, -0.41638708]]   \n",
       "192   [[-0.46955225, -0.37156785, -0.35248777, -0.62343365, -0.45276725]]   \n",
       "88                                                                   None   \n",
       "430     [[-0.3030023, -0.14486581, -0.21068025, -0.15728547, -0.2243253]]   \n",
       "\n",
       "      label  concept_label  \\\n",
       "1177      1  [[1 0 0 1 1]]   \n",
       "1238      0  [[0 0 0 0 0]]   \n",
       "774       0  [[0 0 0 0 0]]   \n",
       "85        1  [[1 0 0 1 1]]   \n",
       "993       0  [[0 0 0 0 0]]   \n",
       "1746      0  [[0 0 0 0 0]]   \n",
       "577       1  [[0 1 0 1 0]]   \n",
       "192       0  [[0 0 0 0 0]]   \n",
       "88        0  [[0 0 0 0 0]]   \n",
       "430       0  [[0 0 0 0 0]]   \n",
       "\n",
       "                                                            tcav_score  \n",
       "1177    [0.00255218, -0.00270456, 0.00349239, 0.00188917, -0.00032008]  \n",
       "1238      [0.00198666, 0.00461079, 0.00252969, 0.00196735, 0.00063642]  \n",
       "774   [0.00304911, -0.00279948, -0.00223824, -0.00041029, -0.00215613]  \n",
       "85     [-0.00174784, 0.00041132, 0.00098502, -0.00068852, -0.00106911]  \n",
       "993         [0.00550162, 0.00096809, 0.00549318, 0.004867, 0.00254748]  \n",
       "1746   [0.00450565, -0.00639469, -0.00578705, -0.00338063, 0.00805975]  \n",
       "577   [-0.00172819, -0.00179959, 0.00091168, -0.00319312, -0.00376357]  \n",
       "192       [0.00162354, 0.00743648, 0.00128449, 0.00285041, 0.00210718]  \n",
       "88        [0.00402275, 0.00629554, 0.0015151, 0.00314647, -0.00208538]  \n",
       "430      [0.00380062, 0.00455174, -0.00263239, 0.00392534, 0.00875435]  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de899626-7696-4cff-ab59-11b131ef83f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been transferred successfully!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5560dc58-3420-488e-b820-747b0df738f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 2.2.0",
   "language": "python",
   "name": "torch220"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
